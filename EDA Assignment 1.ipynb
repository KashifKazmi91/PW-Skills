{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5a96b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What are the key features of the wine quality data set? Discuss the importance of each feature in\n",
    "predicting the quality of wine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fced763",
   "metadata": {},
   "outputs": [],
   "source": [
    "- Exploratory Data Analysis (EDA): Understanding the distributions and relationships between features helps in \n",
    "selecting important variables for modeling.\n",
    "- Feature Engineering: Creating interaction terms (e.g., combining fixed and volatile acidity) can enhance model \n",
    "performance by capturing non-linear relationships.\n",
    "- Machine Learning Models: Techniques like regression, decision trees, or ensemble methods (e.g., Random Forests) can \n",
    "be employed to predict wine quality based on the physicochemical properties.\n",
    "- Model Evaluation: Metrics such as RMSE (Root Mean Square Error) for regression tasks or accuracy/F1-score for \n",
    "classification tasks help assess the effectiveness of the model.\n",
    "- Cross-Validation: This technique ensures the model's robustness and helps mitigate overfitting, ensuring reliable \n",
    "predictions across different wine samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320e2be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. How did you handle missing data in the wine quality data set during the feature engineering process?\n",
    "Discuss the advantages and disadvantages of different imputation techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0c8b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "Common Imputation Techniques\n",
    "1. Mean/Median Imputation:\n",
    "   - Description: Replace missing values with the mean or median of the column.\n",
    "   - Advantages:\n",
    "     - Simple to implement and understand.\n",
    "     - Retains the dataset size, preventing loss of information.\n",
    "     - Works well for normally distributed data (mean) or skewed data (median).\n",
    "   - Disadvantages:\n",
    "     - Can distort the data distribution, especially if a significant number of values are missing.\n",
    "     - Does not account for the relationships between features, leading to biased estimates.\n",
    "\n",
    "2. Mode Imputation:\n",
    "   - Description: Replace missing values with the most frequent value (mode).\n",
    "   - Advantages:\n",
    "     - Useful for categorical features.\n",
    "     - Preserves the mode of the dataset.\n",
    "   - Disadvantages:\n",
    "     - Can lead to a reduction in variability.\n",
    "     - May not be effective if the mode is not representative of the data.\n",
    "\n",
    "3. K-Nearest Neighbors (KNN) Imputation:\n",
    "   - Description: Replace missing values based on the values of the nearest neighbors.\n",
    "   - Advantages:\n",
    "     - Accounts for the relationships between features, providing a more accurate estimate.\n",
    "     - Works well with numerical and categorical data.\n",
    "   - Disadvantages:\n",
    "     - Computationally intensive, especially for large datasets.\n",
    "     - The choice of \\( k \\) can significantly affect results, and it can introduce noise if neighbors are not similar.\n",
    "\n",
    "4. Regression Imputation:\n",
    "   - Description: Predict missing values using a regression model based on other features.\n",
    "   - Advantages:\n",
    "     - Utilizes the relationships between features, leading to potentially more accurate imputations.\n",
    "     - Can capture complex patterns in the data.\n",
    "   - Disadvantages:\n",
    "     - Requires additional modeling, increasing complexity.\n",
    "     - Can introduce bias if the regression model is not well-fitted.\n",
    "\n",
    "5. Multiple Imputation:\n",
    "   - Description: Generates multiple datasets by imputing values multiple times and combines results.\n",
    "   - Advantages:\n",
    "     - Provides a measure of uncertainty in the imputed values.\n",
    "     - Better preserves the statistical properties of the dataset.\n",
    "   - Disadvantages:\n",
    "     - More complex to implement and interpret.\n",
    "     - Increased computational demands due to multiple analyses.\n",
    "\n",
    "6. Removing Missing Values:\n",
    "   - Description: Exclude rows or columns with missing values.\n",
    "   - Advantages:\n",
    "     - Simplest approach; no need for imputation.\n",
    "     - Ensures data integrity by removing potentially unreliable data.\n",
    "   - Disadvantages:\n",
    "     - Can lead to loss of valuable information, especially if many rows or critical features are dropped.\n",
    "     - Can introduce bias if the missingness is not random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cbc3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What are the key factors that affect students' performance in exams? How would you go about\n",
    "analyzing these factors using statistical techniques?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b348dd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Analyzing factors that affect students' performance in exams involves identifying relevant variables, collecting data,\n",
    "and applying appropriate statistical techniques. Here are some key factors that can influence student performance, \n",
    "along with a structured approach to analysis:\n",
    "\n",
    "### Key Factors Affecting Student Performance\n",
    "1. **Socioeconomic Status**:\n",
    "   - Access to resources (tutoring, books, technology).\n",
    "   - Family support and education level.\n",
    "\n",
    "2. **Study Habits**:\n",
    "   - Time spent studying.\n",
    "   - Use of effective study techniques (e.g., practice tests, summarization).\n",
    "\n",
    "3. **Attendance**:\n",
    "   - Frequency of class attendance.\n",
    "   - Engagement in classroom activities.\n",
    "\n",
    "4. **Mental and Physical Health**:\n",
    "   - Stress levels and mental health conditions.\n",
    "   - Physical health and nutrition.\n",
    "\n",
    "5. **Parental Involvement**:\n",
    "   - Support with homework and study.\n",
    "   - Communication with teachers.\n",
    "\n",
    "6. **Teaching Quality**:\n",
    "   - Qualifications and experience of teachers.\n",
    "   - Teaching methods used.\n",
    "\n",
    "7. **Peer Influence**:\n",
    "   - Study groups and collaboration.\n",
    "   - Competitive or supportive peer relationships.\n",
    "\n",
    "8. **Motivation and Attitude**:\n",
    "   - Intrinsic motivation for learning.\n",
    "   - Attitude towards exams and subjects.\n",
    "\n",
    "### Statistical Techniques for Analysis\n",
    "\n",
    "1. **Descriptive Statistics**:\n",
    "   - Use mean, median, mode, and standard deviation to summarize the data.\n",
    "   - Visualize distributions through histograms and box plots to identify trends and outliers.\n",
    "\n",
    "2. **Correlation Analysis**:\n",
    "   - Calculate correlation coefficients (e.g., Pearson or Spearman) to assess relationships between continuous \n",
    "variables (e.g., study time vs. exam scores).\n",
    "\n",
    "3. **Regression Analysis**:\n",
    "   - **Multiple Linear Regression**: Model the relationship between multiple independent variables (e.g., study habits,\n",
    "attendance) and the dependent variable (exam performance).\n",
    "   - **Logistic Regression**: If predicting categorical outcomes (e.g., pass/fail), use logistic regression to estimate \n",
    "probabilities.\n",
    "\n",
    "4. **ANOVA (Analysis of Variance)**:\n",
    "   - Use ANOVA to compare means across different groups (e.g., performance based on different teaching methods or \n",
    "parental involvement levels).\n",
    "\n",
    "5. **Factor Analysis**:\n",
    "   - Identify underlying relationships among variables by grouping them into factors, helping to simplify the analysis.\n",
    "\n",
    "6. **Chi-Squared Test**:\n",
    "   - Use for categorical data to examine relationships between variables (e.g., parental involvement level and \n",
    "pass/fail status).\n",
    "\n",
    "7. **Machine Learning Techniques**:\n",
    "   - Apply decision trees or random forests to explore complex interactions between factors and predict student \n",
    "performance.\n",
    "\n",
    "### Steps to Conduct the Analysis\n",
    "\n",
    "1. **Data Collection**:\n",
    "   - Gather data from surveys, academic records, and demographic information.\n",
    "\n",
    "2. **Data Cleaning**:\n",
    "   - Handle missing values, outliers, and inconsistencies.\n",
    "\n",
    "3. **Exploratory Data Analysis (EDA)**:\n",
    "   - Conduct EDA to identify patterns, relationships, and distributions among variables.\n",
    "\n",
    "4. **Hypothesis Testing**:\n",
    "   - Formulate and test hypotheses regarding the impact of specific factors on student performance.\n",
    "\n",
    "5. **Model Building**:\n",
    "   - Choose appropriate statistical or machine learning models based on the data and research questions.\n",
    "\n",
    "6. **Interpretation**:\n",
    "   - Analyze the results to draw conclusions about the impact of different factors on exam performance.\n",
    "\n",
    "7. **Reporting**:\n",
    "   - Communicate findings through visualizations and written reports, highlighting key insights and recommendations \n",
    "for educators and policymakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17619e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. Describe the process of feature engineering in the context of the student performance data set. How\n",
    "did you select and transform the variables for your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d985992",
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature engineering is a crucial step in preparing data for modeling, especially in the context of a student \n",
    "performance dataset. It involves selecting, transforming, and creating new variables to enhance the predictive \n",
    "power of your model. Here’s a structured approach to feature engineering in this context:\n",
    "\n",
    "### 1. **Understanding the Dataset**\n",
    "Start by gaining a thorough understanding of the dataset, which may include variables such as:\n",
    "- Demographics (age, gender, socioeconomic status)\n",
    "- Academic records (previous grades, attendance)\n",
    "- Behavioral factors (study habits, extracurricular activities)\n",
    "- Psychological factors (motivation, stress levels)\n",
    "\n",
    "### 2. **Data Cleaning**\n",
    "\n",
    "- **Handle Missing Values**: Identify and impute missing values using techniques such as mean/mode imputation or \n",
    "    more sophisticated methods like KNN imputation.\n",
    "- **Remove Duplicates**: Check for and eliminate duplicate records to maintain data integrity.\n",
    "- **Correct Errors**: Identify and rectify inconsistencies (e.g., incorrect grades or demographic information).\n",
    "\n",
    "### 3. **Variable Selection**\n",
    "\n",
    "- **Correlation Analysis**: Use correlation matrices to identify relationships between features and the target \n",
    "    variable (e.g., exam scores). Remove features with low correlation.\n",
    "- **Domain Knowledge**: Leverage insights from educational psychology and pedagogy to select variables that are \n",
    "    theoretically important for student performance.\n",
    "- **Feature Importance**: Utilize techniques such as Random Forests to assess feature importance, helping prioritize \n",
    "    variables for modeling.\n",
    "\n",
    "### 4. **Variable Transformation**\n",
    "\n",
    "- **Normalization/Standardization**: Scale features like study time or attendance rates to ensure they are on a \n",
    "    similar scale, improving model performance.\n",
    "- **Categorical Encoding**: Convert categorical variables (e.g., gender, parental involvement) into numerical format \n",
    "    using techniques like one-hot encoding or label encoding.\n",
    "- **Binning**: Create bins for continuous variables (e.g., categorizing study time into ‘low’, ‘medium’, ‘high’) to \n",
    "    simplify relationships and reduce noise.\n",
    "- **Polynomial Features**: Generate interaction terms or polynomial features if relationships between variables are \n",
    "    expected to be non-linear.\n",
    "\n",
    "### 5. **Creating New Features**\n",
    "\n",
    "- **Aggregate Features**: Combine multiple related variables into a single feature (e.g., average grades across \n",
    "subjects).\n",
    "- **Behavioral Metrics**: Create composite scores based on various behavioral factors (e.g., a \"study engagement\" \n",
    "score that combines study habits and attendance).\n",
    "- **Time-Based Features**: If time-related data is available, derive features such as \"time to exam\" or \"study hours \n",
    "per week.\"\n",
    "\n",
    "### 6. **Feature Selection**\n",
    "\n",
    "- **Recursive Feature Elimination**: Use this technique to iteratively remove features and select the best-performing \n",
    "subset.\n",
    "- **Cross-Validation**: Validate feature sets using cross-validation to ensure that the selected features contribute \n",
    "positively to model performance.\n",
    "\n",
    "### 7. **Finalizing Features**\n",
    "\n",
    "- **Model Iteration**: After initial modeling, revisit and refine features based on model performance metrics. Keep \n",
    "track of which features significantly improve prediction accuracy.\n",
    "- **Documentation**: Document all transformations and feature selections for reproducibility and transparency.\n",
    "\n",
    "### Example of Variable Selection and Transformation\n",
    "Let’s say the dataset includes the following columns: `study_time`, `attendance`, `parental_support`, `previous_grades`,\n",
    "and `mental_health_score`.\n",
    "- **Selected Variables**: Based on correlation analysis, you might select `study_time`, `attendance`, and \n",
    "`previous_grades` as key predictors.\n",
    "- **Transformations**:\n",
    "  - **Normalization**: Scale `study_time` (e.g., Min-Max scaling).\n",
    "  - **Encoding**: Convert `parental_support` (e.g., “high”, “medium”, “low”) using one-hot encoding.\n",
    "  - **Binning**: Create bins for `previous_grades` to categorize them into performance tiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8e6d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Load the wine quality data set and perform exploratory data analysis (EDA) to identify the distribution\n",
    "of each feature. Which feature(s) exhibit non-normality, and what transformations could be applied to\n",
    "these features to improve normality?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d044856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('wine_quality.csv')\n",
    "\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "print(df.describe())\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot histograms for all features\n",
    "df.hist(bins=30, figsize=(15, 10))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "for column in df.columns:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.histplot(df[column], kde=True)\n",
    "    plt.title(f'Distribution of {column}')\n",
    "    plt.show()\n",
    "\n",
    "    # Q-Q plot\n",
    "    stats.probplot(df[column], dist=\"norm\", plot=plt)\n",
    "    plt.title(f'Q-Q plot for {column}')\n",
    "    plt.show()\n",
    "\n",
    "    # Shapiro-Wilk test\n",
    "    stat, p = stats.shapiro(df[column])\n",
    "    print(f'Shapiro-Wilk test for {column}: Statistics={stat}, p-value={p}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a7c9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Log Transformation\n",
    "df['log_volatile_acidity'] = np.log1p(df['volatile_acidity'])  # log(1+x) to handle zero values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8274d795",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Square Root Transformation\n",
    "df['sqrt_fixed_acidity'] = np.sqrt(df['fixed_acidity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17657c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Box-Cox Transformation\n",
    "from scipy import stats\n",
    "\n",
    "df['boxcox_residual_sugar'], _ = stats.boxcox(df['residual_sugar'] + 1)  # Adding 1 to handle zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fac7363",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Yeo-Johnson Transformation\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "df[['transformed_chlorides']] = pt.fit_transform(df[['chlorides']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4dc4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Using the wine quality data set, perform principal component analysis (PCA) to reduce the number of\n",
    "features. What is the minimum number of principal components required to explain 90% of the variance in\n",
    "the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c85ebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('wine_quality.csv')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df.drop('quality', axis=1)  # Assuming 'quality' is the target variable\n",
    "y = df['quality']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Initialize PCA\n",
    "pca = PCA()\n",
    "\n",
    "# Fit PCA on the scaled data\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot explained variance ratio\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, marker='o')\n",
    "plt.title('Explained Variance Ratio by Principal Component')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "cumulative_variance = pca.explained_variance_ratio_.cumsum()\n",
    "\n",
    "# Find the number of components for 90% variance\n",
    "num_components_90 = (cumulative_variance >= 0.90).argmax() + 1\n",
    "print(f'Minimum number of principal components required to explain 90% of the variance: {num_components_90}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
