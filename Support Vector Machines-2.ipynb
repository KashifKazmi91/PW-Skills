{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c349b11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a1c95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "In machine learning, particularly in algorithms like Support Vector Machines (SVM), there is a significant relationship\n",
    "between polynomial functions and kernel functions. Here's a detailed explanation of that relationship:\n",
    "\n",
    "### 1. **Polynomial Functions**\n",
    "\n",
    "A polynomial function is a mathematical expression of the form:\n",
    "\n",
    "\\[\n",
    "f(x) = a_n x^n + a_{n-1} x^{n-1} + \\ldots + a_1 x + a_0\n",
    "\\]\n",
    "\n",
    "where \\(a_n, a_{n-1}, \\ldots, a_0\\) are coefficients, and \\(n\\) is a non-negative integer indicating the degree of\n",
    "the polynomial. Polynomial functions can model complex relationships between variables, especially in higher dimensions.\n",
    "\n",
    "### 2. **Kernel Functions**\n",
    "\n",
    "Kernel functions are used in machine learning to enable algorithms to operate in higher-dimensional spaces without \n",
    "explicitly transforming the data. A kernel function computes the inner product of two vectors in this high-dimensional\n",
    "space, allowing algorithms like SVM to learn non-linear decision boundaries.\n",
    "\n",
    "### 3. **Polynomial Kernel Function**\n",
    "\n",
    "One specific type of kernel function is the polynomial kernel, which is defined as:\n",
    "\n",
    "\\[\n",
    "K(x_i, x_j) = (x_i \\cdot x_j + c)^d\n",
    "\\]\n",
    "\n",
    "where:\n",
    "- \\(x_i\\) and \\(x_j\\) are input feature vectors.\n",
    "- \\(c\\) is a constant (often set to 1).\n",
    "- \\(d\\) is the degree of the polynomial.\n",
    "\n",
    "### 4. **Relationship**\n",
    "\n",
    "- **Transformation to Higher Dimensions**: The polynomial kernel implicitly maps the input features into a \n",
    "    higher-dimensional space where polynomial relationships can be modeled. For example, a second-degree polynomial\n",
    "    kernel can create combinations of features such as \\(x_1^2\\), \\(x_2^2\\), and \\(x_1 x_2\\) without needing to compute\n",
    "    these combinations explicitly.\n",
    "\n",
    "- **Flexibility in Modeling**: By using polynomial kernels, machine learning algorithms can capture non-linear \n",
    "    relationships between features. The degree of the polynomial \\(d\\) allows the model to control the complexity.\n",
    "    Higher degrees enable the model to fit more complex patterns.\n",
    "\n",
    "- **Efficiency**: Using kernel functions, including polynomial kernels, allows algorithms to learn complex \n",
    "    relationships efficiently. Instead of explicitly computing the transformed features, the kernel computes \n",
    "    the necessary inner products directly, reducing computational overhead.\n",
    "\n",
    "### 5. **Practical Use in SVM**\n",
    "\n",
    "In SVM, using a polynomial kernel allows the classifier to create non-linear decision boundaries based on the \n",
    "polynomial relationships between the features. By adjusting the degree of the polynomial, practitioners can control\n",
    "the model's capacity to fit the training data, balancing bias and variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad8903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599cb458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # Use only the first two features for easy visualization\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into a training set and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create an SVM model with a polynomial kernel\n",
    "degree = 3  # Degree of the polynomial\n",
    "svm_model = SVC(kernel='poly', degree=degree, C=1.0, coef0=1)  # coef0 is a constant term in the kernel\n",
    "\n",
    "# Train the model\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:\\n', confusion)\n",
    "\n",
    "# Plotting decision boundaries\n",
    "def plot_decision_boundaries(model, X, y):\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
    "                         np.arange(y_min, y_max, 0.01))\n",
    "\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o')\n",
    "    plt.title(f'SVM with Polynomial Kernel (degree={degree})')\n",
    "    plt.xlabel(iris.feature_names[0])\n",
    "    plt.ylabel(iris.feature_names[1])\n",
    "    plt.show()\n",
    "\n",
    "# Plot the decision boundaries\n",
    "plot_decision_boundaries(svm_model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0facda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be99a2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "In Support Vector Regression (SVR), the parameter \\(\\epsilon\\) (epsilon) defines the width of the margin around the \n",
    "regression function within which no penalty is assigned to errors. Here's how increasing the value of \\(\\epsilon\\)\n",
    "affects the number of support vectors:\n",
    "\n",
    "### 1. **Understanding Epsilon**\n",
    "\n",
    "- **Epsilon in SVR**: The \\(\\epsilon\\) parameter creates a \"tube\" around the regression line (or hyperplane in higher\n",
    "dimensions). Any data points that fall within this tube are considered to be correctly predicted and incur no loss. \n",
    "Points outside this tube contribute to the loss and become support vectors.\n",
    "\n",
    "### 2. **Effect of Increasing Epsilon**\n",
    "\n",
    "- **Wider Margin**: As \\(\\epsilon\\) increases, the width of the margin increases. This means that more points are \n",
    "    likely to fall within the margin (the \\(\\epsilon\\)-tube).\n",
    "  \n",
    "- **Fewer Support Vectors**: With a wider margin, fewer points will be considered as support vectors. Since support\n",
    "    vectors are defined as the points outside the \\(\\epsilon\\) margin that contribute to the loss, a larger \\(\\epsilon\\)\n",
    "    will result in fewer points being outside this margin.\n",
    "\n",
    "### 3. **Implications of Fewer Support Vectors**\n",
    "\n",
    "- **Reduced Complexity**: Fewer support vectors can lead to a simpler model, which may improve generalization on \n",
    "    unseen data. This can be beneficial in scenarios where noise is present in the data.\n",
    "\n",
    "- **Potential Loss of Detail**: While increasing \\(\\epsilon\\) reduces the number of support vectors, it may also\n",
    "    mean that the model could overlook important patterns or variations in the data, particularly if \\(\\epsilon\\) \n",
    "    is set too high.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf4a182",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter\n",
    "affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works\n",
    "and provide examples of when you might want to increase or decrease its value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c31a3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "The performance of Support Vector Regression (SVR) is significantly influenced by several key parameters: the choice \n",
    "    of kernel function, the \\(C\\) parameter, the \\(\\epsilon\\) parameter, and the \\(\\gamma\\) parameter \n",
    "    (in the case of certain kernels like RBF). Hereâ€™s a detailed explanation of each parameter, how they work, \n",
    "    and when you might want to adjust their values.\n",
    "\n",
    "### 1. **Choice of Kernel Function**\n",
    "\n",
    "**How it Works**:\n",
    "- The kernel function transforms the input data into a higher-dimensional space to enable non-linear relationships\n",
    "to be modeled. Common kernels include:\n",
    "  - **Linear**: Assumes a linear relationship.\n",
    "  - **Polynomial**: Captures polynomial relationships (controlled by the degree).\n",
    "  - **RBF (Radial Basis Function)**: Captures more complex relationships and is commonly used for non-linear data.\n",
    "\n",
    "**When to Adjust**:\n",
    "- **Use Linear Kernel**: When the data is approximately linear. It is computationally cheaper and simpler.\n",
    "- **Use RBF Kernel**: When the data exhibits non-linear relationships, as it can model more complex patterns.\n",
    "- **Use Polynomial Kernel**: When you suspect polynomial relationships; adjust the degree for complexity.\n",
    "\n",
    "### 2. **C Parameter**\n",
    "\n",
    "**How it Works**:\n",
    "- The \\(C\\) parameter controls the trade-off between maximizing the margin and minimizing the regression error.\n",
    "A small \\(C\\) allows for a wider margin but may accept more errors, while a large \\(C\\) attempts to minimize errors\n",
    "more strictly, potentially leading to overfitting.\n",
    "\n",
    "**When to Adjust**:\n",
    "- **Increase \\(C\\)**: When you want to prioritize accuracy over model simplicity, especially if your data has few \n",
    "    outliers.\n",
    "- **Decrease \\(C\\)**: When you have noisy data or want to create a more generalized model that does not fit the \n",
    "    training data too closely.\n",
    "\n",
    "### 3. **Epsilon Parameter (\\(\\epsilon\\))**\n",
    "\n",
    "**How it Works**:\n",
    "- The \\(\\epsilon\\) parameter defines a margin of tolerance where no penalty is given to errors. It determines the \n",
    "width of the \\(\\epsilon\\)-tube around the predicted values. Points outside this tube contribute to the loss.\n",
    "\n",
    "**When to Adjust**:\n",
    "- **Increase \\(\\epsilon\\)**: When you want to ignore more minor deviations from the predicted values, especially in \n",
    "    noisy datasets. This leads to fewer support vectors.\n",
    "- **Decrease \\(\\epsilon\\)**: When you want to capture more details in the data, particularly if your dataset is clean\n",
    "    and you want to minimize error.\n",
    "\n",
    "### 4. **Gamma Parameter (\\(\\gamma\\))**\n",
    "\n",
    "**How it Works**:\n",
    "- The \\(\\gamma\\) parameter is specific to the RBF and polynomial kernels. It defines how far the influence of a single\n",
    "training example reaches. A low \\(\\gamma\\) means a far reach (smooth decision boundary), while a high \\(\\gamma\\) means\n",
    "a close reach (complex decision boundary).\n",
    "\n",
    "**When to Adjust**:\n",
    "- **Increase \\(\\gamma\\)**: When you want the model to capture more complex patterns. This can lead to overfitting if \n",
    "    too high.\n",
    "- **Decrease \\(\\gamma\\)**: When you want a smoother decision boundary that generalizes better to unseen data, \n",
    "    particularly if the data is noisy.\n",
    "\n",
    "### Summary of Effects\n",
    "\n",
    "| Parameter         | Effect on Model Performance                                   | When to Adjust                             |\n",
    "|-------------------|---------------------------------------------------------------|--------------------------------------------|\n",
    "| Kernel Function   | Determines the type of relationship modeled                   | Choose based on data linearity            |\n",
    "| \\(C\\)             | Trade-off between margin size and error minimization          | Increase for more accuracy; decrease for generalization |\n",
    "| \\(\\epsilon\\)      | Width of the margin for ignoring errors                       | Increase for noise; decrease for detail    |\n",
    "| \\(\\gamma\\)        | Influence reach of training samples in non-linear kernels     | Increase for complexity; decrease for smoothness |\n",
    "\n",
    "### Examples\n",
    "\n",
    "- **Linear Kernel with High \\(C\\)**: Useful for clean, linear datasets where accuracy is crucial.\n",
    "- **RBF Kernel with Low \\(\\epsilon\\) and High \\(\\gamma\\)**: Good for complex, noisy datasets where you want to capture\n",
    "    fine details.\n",
    "- **Polynomial Kernel with Moderate \\(C\\)**: Suitable for datasets with polynomial trends but where you want some \n",
    "    tolerance for noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a39e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Assignment:\n",
    "Import the necessary libraries and load the dataseg\n",
    "Split the dataset into training and testing setZ\n",
    "Preprocess the data using any technique of your choice (e.g. scaling, normalizationK\n",
    "Create an instance of the SVC classifier and train it on the training datW\n",
    "Use the trained classifier to predict the labels of the testing datW\n",
    "Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,\n",
    "precision, recall, F1-scoreK\n",
    "Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomizedSearchCV to\n",
    "improve its performanc_\n",
    "Train the tuned classifier on the entire dataseg\n",
    "Save the trained classifier to a file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d111a1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import joblib  # For saving the model\n",
    "\n",
    "# 1. Load the dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# 2. Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 3. Preprocess the data (scaling)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 4. Create an instance of the SVC classifier and train it on the training data\n",
    "svc = SVC(kernel='rbf', C=1.0, gamma='scale')  # Using RBF kernel with default parameters\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 5. Use the trained classifier to predict the labels of the testing data\n",
    "y_pred = svc.predict(X_test_scaled)\n",
    "\n",
    "# 6. Evaluate the performance of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 7. Tune the hyperparameters using GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.01, 0.1, 1],\n",
    "    'kernel': ['rbf', 'linear']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best parameters from GridSearchCV\n",
    "print(f'Best parameters: {grid_search.best_params_}')\n",
    "\n",
    "# 8. Train the tuned classifier on the entire dataset\n",
    "best_svc = grid_search.best_estimator_\n",
    "best_svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 9. Save the trained classifier to a file\n",
    "joblib.dump(best_svc, 'svm_iris_model.pkl')\n",
    "print(\"Model saved to 'svm_iris_model.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e2cfc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018182c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c25a620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d960136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075d30a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
