{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c353cd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the mathematical formula for a linear SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97047773",
   "metadata": {},
   "outputs": [],
   "source": [
    "The mathematical formulation of a linear Support Vector Machine (SVM) involves finding a hyperplane that best \n",
    "separates two classes in a high-dimensional space. The goal is to maximize the margin between the closest points \n",
    "of the classes, known as support vectors.\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "1. **Hyperplane Equation**: In an \\( n \\)-dimensional space, a hyperplane can be represented as:\n",
    "\n",
    "   \\[\n",
    "   w \\cdot x + b = 0\n",
    "   \\]\n",
    "\n",
    "   where:\n",
    "   - \\( w \\) is the weight vector (normal to the hyperplane).\n",
    "   - \\( x \\) is the feature vector.\n",
    "   - \\( b \\) is the bias term (intercept).\n",
    "\n",
    "2. **Classification Rule**: The SVM classifies data points based on which side of the hyperplane they fall:\n",
    "\n",
    "   \\[\n",
    "   f(x) = \\text{sign}(w \\cdot x + b)\n",
    "   \\]\n",
    "\n",
    "3. **Margin Maximization**: The SVM aims to maximize the margin \\( \\gamma \\), which is defined as the distance \n",
    "    between the hyperplane and the closest data points from either class. This is done subject to the constraints:\n",
    "\n",
    "   \\[\n",
    "   y_i (w \\cdot x_i + b) \\geq 1 \\quad \\forall i\n",
    "   \\]\n",
    "\n",
    "   where \\( y_i \\) is the label of the \\( i \\)-th sample, which can be either +1 or -1.\n",
    "\n",
    "4. **Optimization Problem**: The optimization problem can be expressed as:\n",
    "\n",
    "   \\[\n",
    "   \\min_{w, b} \\frac{1}{2} \\|w\\|^2\n",
    "   \\]\n",
    "\n",
    "   subject to the constraints mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02fa4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is the objective function of a linear SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86aa7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "The objective function of a linear Support Vector Machine (SVM) is focused on maximizing the margin between two \n",
    "classes while ensuring that the data points are correctly classified. This involves minimizing the norm of the weight\n",
    "vector \\( w \\), which is related to the margin, subject to certain constraints. \n",
    "\n",
    "### Objective Function\n",
    "\n",
    "The objective function can be formally stated as:\n",
    "\n",
    "\\[\n",
    "\\min_{w, b} \\frac{1}{2} \\|w\\|^2\n",
    "\\]\n",
    "\n",
    "### Constraints\n",
    "\n",
    "This minimization is subject to the constraints that ensure correct classification of the training samples:\n",
    "\n",
    "\\[\n",
    "y_i (w \\cdot x_i + b) \\geq 1 \\quad \\forall i\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( y_i \\) is the label of the \\( i \\)-th sample, taking values +1 or -1.\n",
    "- \\( x_i \\) is the feature vector of the \\( i \\)-th sample.\n",
    "- \\( w \\) is the weight vector, and \\( b \\) is the bias term.\n",
    "\n",
    "### Explanation\n",
    "\n",
    "1. **Minimizing \\(\\frac{1}{2} \\|w\\|^2\\)**: This term represents the squared magnitude of the weight vector \\( w \\). \n",
    "    Minimizing this term corresponds to maximizing the margin between the hyperplane and the nearest data points \n",
    "    (support vectors). The factor of \\( \\frac{1}{2} \\) is used for mathematical convenience, especially during \n",
    "    optimization.\n",
    "\n",
    "2. **Constraints**: The constraints ensure that each data point is classified correctly with respect to the hyperplane.\n",
    "    Specifically, for points belonging to the positive class (label +1), the inequality should be satisfied positively,\n",
    "    and for points in the negative class (label -1), it should be satisfied negatively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a1e4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is the kernel trick in SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026de4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "The kernel trick is a powerful technique used in Support Vector Machines (SVM) that allows the algorithm to operate \n",
    "in a higher-dimensional space without explicitly computing the coordinates of the data in that space. Instead, it \n",
    "uses a kernel function to compute the inner products between the data points in the transformed feature space. \n",
    "This enables SVMs to effectively handle non-linear relationships between data points.\n",
    "\n",
    "### Key Concepts of the Kernel Trick\n",
    "\n",
    "1. **Non-linearly Separable Data**: In many real-world scenarios, data points from different classes cannot be \n",
    "    separated by a straight line (or hyperplane) in their original feature space. The kernel trick allows SVM to \n",
    "    create a decision boundary that is non-linear in the original space by mapping the data into a higher-dimensional \n",
    "    space.\n",
    "\n",
    "2. **Kernel Function**: A kernel function computes the dot product of two vectors in a higher-dimensional feature \n",
    "    space without explicitly transforming the data. Common kernel functions include:\n",
    "   - **Linear Kernel**: \\( K(x_i, x_j) = x_i \\cdot x_j \\)\n",
    "   - **Polynomial Kernel**: \\( K(x_i, x_j) = (x_i \\cdot x_j + c)^d \\) for constants \\( c \\) and degree \\( d \\)\n",
    "   - **Radial Basis Function (RBF) Kernel**: \\( K(x_i, x_j) = \\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{2\\sigma^2}\\right) \\)\n",
    "   - **Sigmoid Kernel**: \\( K(x_i, x_j) = \\tanh(\\alpha x_i \\cdot x_j + c) \\)\n",
    "\n",
    "3. **Implicit Mapping**: The kernel trick allows SVM to learn complex decision boundaries by implicitly mapping the \n",
    "    original features into a higher-dimensional space. This mapping is performed through the kernel function without \n",
    "    the need to calculate the coordinates of the points in that space.\n",
    "\n",
    "### Benefits of the Kernel Trick\n",
    "\n",
    "- **Computational Efficiency**: By avoiding the explicit transformation of the data, the kernel trick saves \n",
    "    computational resources, making it feasible to work with high-dimensional data.\n",
    "- **Flexibility**: Different kernel functions can be used to suit the nature of the data and the problem at hand,\n",
    "    allowing for a wide variety of decision boundaries.\n",
    "- **Improved Performance**: SVMs can achieve better classification performance on complex datasets that are not \n",
    "    linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad0740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is the role of support vectors in SVM Explain with example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6bc534",
   "metadata": {},
   "outputs": [],
   "source": [
    "Support vectors are the data points that are closest to the decision boundary (hyperplane) in a Support Vector Machine\n",
    "(SVM) model. They play a crucial role in defining the optimal hyperplane that separates different classes. Here’s a \n",
    "detailed explanation of their role, along with an example:\n",
    "\n",
    "### Role of Support Vectors\n",
    "\n",
    "1. **Defining the Decision Boundary**: Support vectors are the critical elements of the training dataset that determine\n",
    "    the position and orientation of the hyperplane. If any other points were removed (excluding the support vectors), \n",
    "    the position of the hyperplane would change.\n",
    "\n",
    "2. **Maximizing the Margin**: The SVM aims to maximize the margin between the hyperplane and the closest points from \n",
    "    each class. The distance from the hyperplane to the support vectors is crucial because the SVM algorithm focuses \n",
    "    on maximizing this margin.\n",
    "\n",
    "3. **Robustness**: Because the decision boundary is determined by only a subset of the data (the support vectors), \n",
    "    SVM can be more robust to outliers and noise in the dataset. Points that are far from the decision boundary do \n",
    "    not influence the position of the hyperplane.\n",
    "\n",
    "### Example\n",
    "\n",
    "Imagine a 2D dataset with two classes (red and blue) that can be separated by a line (hyperplane). Here’s how support \n",
    "vectors would work in this context:\n",
    "\n",
    "1. **Visual Representation**:\n",
    "   - Suppose we have the following points:\n",
    "     - Red points: (1, 2), (2, 3), (3, 3)\n",
    "     - Blue points: (5, 5), (6, 5), (7, 6)\n",
    "   \n",
    "   If we plot these points, we might find that the optimal hyperplane that separates these two classes could be \n",
    "    between the red points and blue points.\n",
    "\n",
    "2. **Identifying Support Vectors**:\n",
    "   - The support vectors are the points that are closest to the hyperplane. In this example, let’s say the red point\n",
    "(2, 3) and the blue point (5, 5) are the closest points to the hyperplane.\n",
    "   - These points are critical because if we were to move them or change their position, the hyperplane would adjust\n",
    "    accordingly.\n",
    "\n",
    "3. **Maximizing the Margin**:\n",
    "   - The SVM would calculate the margin (the distance from the hyperplane to the support vectors). The aim is to \n",
    "maximize this margin.\n",
    "   - The support vectors directly influence the width of the margin. For example, if we add a new red point (2, 2)\n",
    "    that is closer to the hyperplane than the existing support vector (2, 3), it could become the new support vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525aaee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in\n",
    "SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47632747",
   "metadata": {},
   "outputs": [],
   "source": [
    "To illustrate the concepts of hyperplane, margin planes, soft margin, and hard margin in Support Vector Machines (SVM),\n",
    "we can break these down with examples and graphical representations.\n",
    "\n",
    "### 1. Hyperplane\n",
    "\n",
    "A **hyperplane** is a decision boundary that separates different classes in the feature space. In a 2D space, it is\n",
    "simply a line. For example:\n",
    "\n",
    "- **Example**: Consider a dataset with two features \\(x_1\\) and \\(x_2\\), and two classes: red and blue.\n",
    "\n",
    "**Graph**:\n",
    "![Hyperplane](https://i.imgur.com/3Z2p0vU.png)\n",
    "\n",
    "In this plot, the solid line represents the hyperplane that separates the red points from the blue points.\n",
    "\n",
    "### 2. Margin Planes\n",
    "\n",
    "The **margins** are the boundaries that lie parallel to the hyperplane, marking the closest points (support vectors)\n",
    "from each class. The region between the two margin planes is where the SVM tries to maximize the distance.\n",
    "\n",
    "- **Example**: In the same dataset, the margin planes would be lines parallel to the hyperplane that are closest to \n",
    "    the red and blue points.\n",
    "\n",
    "**Graph**:\n",
    "![Margin Planes](https://i.imgur.com/MHGg7Y6.png)\n",
    "\n",
    "Here, the dashed lines represent the margin planes. The distance between the hyperplane and these margins is maximized\n",
    "to achieve the best separation.\n",
    "\n",
    "### 3. Hard Margin SVM\n",
    "\n",
    "A **hard margin** SVM assumes that the data is linearly separable without any noise. All data points must lie on the \n",
    "correct side of the margin planes.\n",
    "\n",
    "- **Example**: When the classes are perfectly separable.\n",
    "\n",
    "**Graph**:\n",
    "![Hard Margin](https://i.imgur.com/HQPIyg5.png)\n",
    "\n",
    "In this graph, all points are correctly classified, and there is a clear gap (the margin) between the classes, with \n",
    "no data points lying within the margin area.\n",
    "\n",
    "### 4. Soft Margin SVM\n",
    "\n",
    "A **soft margin** SVM allows for some misclassification in the case where the data is not perfectly separable. \n",
    "This introduces a trade-off between maximizing the margin and minimizing classification errors.\n",
    "\n",
    "- **Example**: When some points are close to or within the margin.\n",
    "\n",
    "**Graph**:\n",
    "![Soft Margin](https://i.imgur.com/c73RT5g.png)\n",
    "\n",
    "In this plot, the hyperplane is adjusted to account for the misclassified points (indicated by the different colors). \n",
    "The soft margin allows some data points to fall within the margin or even be misclassified, providing flexibility to \n",
    "handle noise and overlapping classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aa88d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. SVM Implementation through Iris dataset.\n",
    "~ Load the iris dataset from the scikit-learn library and split it into a training set and a testing setl\n",
    "~ Train a linear SVM classifier on the training set and predict the labels for the testing setl\n",
    "~ Compute the accuracy of the model on the testing setl\n",
    "~ Plot the decision boundaries of the trained model using two of the featuresl\n",
    "~ Try different values of the regularisation parameter C and see how it affects the performance of\n",
    "the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebefd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # Using only the first two features for visualization\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into a training set and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a linear SVM classifier\n",
    "svm_model = SVC(kernel='linear', C=1.0)  # Using C=1.0 as default\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the testing set\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Plotting decision boundaries\n",
    "def plot_decision_boundaries(model, X, y):\n",
    "    # Create a mesh grid\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
    "                         np.arange(y_min, y_max, 0.01))\n",
    "    \n",
    "    # Predict the function value for the whole grid\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o')\n",
    "    plt.title('SVM Decision Boundaries with Linear Kernel')\n",
    "    plt.xlabel(iris.feature_names[0])\n",
    "    plt.ylabel(iris.feature_names[1])\n",
    "    plt.show()\n",
    "\n",
    "# Plot the decision boundaries\n",
    "plot_decision_boundaries(svm_model, X, y)\n",
    "\n",
    "# Try different values of the regularization parameter C\n",
    "C_values = [0.01, 0.1, 1.0, 10, 100]\n",
    "accuracies = []\n",
    "\n",
    "for C in C_values:\n",
    "    svm_model = SVC(kernel='linear', C=C)\n",
    "    svm_model.fit(X_train, y_train)\n",
    "    y_pred = svm_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f'Accuracy with C={C}: {accuracy:.4f}')\n",
    "\n",
    "# Plot the effect of C on accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(C_values, accuracies, marker='o')\n",
    "plt.xscale('log')\n",
    "plt.title('Effect of Regularization Parameter C on SVM Accuracy')\n",
    "plt.xlabel('C (Regularization Parameter)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(C_values)\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
