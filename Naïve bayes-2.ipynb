{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93acedfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b27111f",
   "metadata": {},
   "outputs": [],
   "source": [
    "The probability that an employee is a smoker given that they use the health insurance plan is **40%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12102a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de3b815",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bernoulli Naive Bayes and Multinomial Naive Bayes are both types of Naive Bayes classifiers, but they differ in how \n",
    "they handle input features and the nature of the data they are designed to work with. Here are the key differences:\n",
    "\n",
    "### 1. **Nature of Features**:\n",
    "- **Bernoulli Naive Bayes**:\n",
    "  - Designed for binary features, where each feature is either present (1) or absent (0).\n",
    "  - Commonly used when the data is in a binary format, such as text classification where the presence or absence of \n",
    "    certain words is relevant.\n",
    "\n",
    "- **Multinomial Naive Bayes**:\n",
    "  - Designed for discrete count features, where features represent counts of occurrences, such as word frequencies \n",
    "in documents.\n",
    "  - Suitable for problems where you want to account for the number of times a feature occurs, such as how many times\n",
    "    a word appears in a document.\n",
    "\n",
    "### 2. **Probability Calculation**:\n",
    "- **Bernoulli Naive Bayes**:\n",
    "  - Assumes a Bernoulli distribution for the features. The model estimates the probability of a feature being present\n",
    "given the class.\n",
    "  - The probability of a class given a feature vector is calculated based on whether each feature is present or absent,\n",
    "    regardless of the number of occurrences.\n",
    "\n",
    "- **Multinomial Naive Bayes**:\n",
    "  - Assumes a multinomial distribution for the features. The model estimates the probability of a feature's occurrence\n",
    "given the class, taking into account the frequency of that feature.\n",
    "  - The probability of a class is calculated based on the counts of each feature.\n",
    "\n",
    "### 3. **Use Cases**:\n",
    "- **Bernoulli Naive Bayes**:\n",
    "  - Often used in document classification tasks where the presence of words is more important than their frequency. \n",
    "For example, classifying emails as spam or not based on the presence of specific keywords.\n",
    "\n",
    "- **Multinomial Naive Bayes**:\n",
    "  - Commonly used in scenarios where the count of features matters, such as text classification where the number of \n",
    "occurrences of words influences the classification. For example, it can be effective in topic modeling or sentiment \n",
    "analysis where word counts are important.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9e4e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4172a6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bernoulli Naive Bayes does not inherently have a built-in mechanism to handle missing values. If your dataset contains\n",
    "missing values, there are several common approaches you can take before applying the Bernoulli Naive Bayes classifier:\n",
    "\n",
    "### 1. **Imputation**:\n",
    "   - **Mean/Median/Mode Imputation**: For numeric features, you can fill in missing values with the mean or median. \n",
    "        For binary features, the mode (most frequent value) is often used.\n",
    "   - **Predictive Imputation**: Use other features in the dataset to predict and fill in missing values using \n",
    "    regression or other models.\n",
    "\n",
    "### 2. **Remove Missing Values**:\n",
    "   - **Row Removal**: If the proportion of missing data is small, you might consider removing rows (instances) with \n",
    "        missing values. However, this can lead to loss of valuable data.\n",
    "   - **Feature Removal**: If a particular feature has a high percentage of missing values, you may choose to remove \n",
    "    that feature entirely if itâ€™s deemed unimportant.\n",
    "\n",
    "### 3. **Assigning a Default Value**:\n",
    "   - You could assign a default value (like 0 for binary features) to represent the absence of a feature. This approach\n",
    "    is simple but may introduce bias if not handled carefully.\n",
    "\n",
    "### 4. **Using a Flag Variable**:\n",
    "   - Create a new binary feature that indicates whether the original feature was missing or not. This way, you retain \n",
    "    information about the presence of missing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee29a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655c531c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. In fact, Naive Bayes classifiers, including \n",
    "Gaussian Naive Bayes, are inherently capable of handling multi-class problems. Here's how it works:\n",
    "\n",
    "### How Gaussian Naive Bayes Handles Multi-Class Classification:\n",
    "\n",
    "1. **Class Probabilities**:\n",
    "   - For a multi-class classification problem with classes \\( C_1, C_2, \\ldots, C_k \\), Gaussian Naive Bayes calculates\n",
    "the prior probabilities \\( P(C_i) \\) for each class \\( C_i \\).\n",
    "\n",
    "2. **Feature Probabilities**:\n",
    "   - For each class, it assumes that the features follow a Gaussian (normal) distribution. For each feature \\( X_j \\),\n",
    "it calculates the mean \\( \\mu_{ij} \\) and variance \\( \\sigma^2_{ij} \\) for that feature within each class \\( C_i \\).\n",
    "\n",
    "3. **Prediction**:\n",
    "   - Given a new instance with features \\( X = (X_1, X_2, \\ldots, X_n) \\), Gaussian Naive Bayes computes the posterior\n",
    "probability for each class using Bayes' theorem:\n",
    "   \\[\n",
    "   P(C_i | X) \\propto P(C_i) \\cdot P(X | C_i)\n",
    "   \\]\n",
    "   - The term \\( P(X | C_i) \\) is calculated as the product of the probabilities of each feature given the class, \n",
    "assuming independence:\n",
    "   \\[\n",
    "   P(X | C_i) = \\prod_{j=1}^{n} P(X_j | C_i)\n",
    "   \\]\n",
    "   - Each feature probability \\( P(X_j | C_i) \\) is calculated using the Gaussian probability density function:\n",
    "   \\[\n",
    "   P(X_j | C_i) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_{ij}}} \\exp\\left(-\\frac{(X_j - \\mu_{ij})^2}{2\\sigma^2_{ij}}\\right)\n",
    "   \\]\n",
    "\n",
    "4. **Class Decision**:\n",
    "   - The class with the highest posterior probability \\( P(C_i | X) \\) is chosen as the predicted class for the new\n",
    "instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1688cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Assignment:\n",
    "Data preparation:\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "Implementation:\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "Results:\n",
    "Report the following performance metrics for each classifier:\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1 score\n",
    "Discussion:\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "Conclusion:\n",
    "Summarise your findings and provide some suggestions for future work.\n",
    "\n",
    "Note: This dataset contains a binary classification problem with multiple features. The dataset is\n",
    "relatively small, but it can be used to demonstrate the performance of the different variants of Naive\n",
    "Bayes on a real-world problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7d1728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/spambase.data\"\n",
    "column_names = [f'feature_{i}' for i in range(1, 58)] + ['is_spam']\n",
    "data = pd.read_csv(url, header=None, names=column_names)\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# Initialize classifiers\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n",
    "\n",
    "# Function to evaluate classifiers\n",
    "def evaluate_classifier(classifier):\n",
    "    scores = cross_val_score(classifier, X, y, cv=10, scoring='accuracy')\n",
    "    accuracy = np.mean(scores)\n",
    "\n",
    "    # Fit the model to get predictions for metrics\n",
    "    classifier.fit(X, y)\n",
    "    predictions = classifier.predict(X)\n",
    "\n",
    "    precision = precision_score(y, predictions)\n",
    "    recall = recall_score(y, predictions)\n",
    "    f1 = f1_score(y, predictions)\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Evaluate each classifier\n",
    "results = {\n",
    "    \"Bernoulli Naive Bayes\": evaluate_classifier(bernoulli_nb),\n",
    "    \"Multinomial Naive Bayes\": evaluate_classifier(multinomial_nb),\n",
    "    \"Gaussian Naive Bayes\": evaluate_classifier(gaussian_nb)\n",
    "}\n",
    "\n",
    "# Print results\n",
    "for classifier, metrics in results.items():\n",
    "    print(f\"{classifier}:\\n\"\n",
    "          f\"Accuracy: {metrics[0]:.4f}\\n\"\n",
    "          f\"Precision: {metrics[1]:.4f}\\n\"\n",
    "          f\"Recall: {metrics[2]:.4f}\\n\"\n",
    "          f\"F1 Score: {metrics[3]:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d0e8cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
