{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee367677",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64532c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Min-Max scaling is a normalization technique used in data preprocessing to transform features to a fixed range, \n",
    "typically [0, 1]. This scaling method is particularly useful when features have different units or scales, as it \n",
    "helps to ensure that each feature contributes equally to the distance computations in algorithms that rely on\n",
    "distance metrics (like k-nearest neighbors or gradient descent).\n",
    "\n",
    "### How Min-Max Scaling Works\n",
    "\n",
    "The Min-Max scaling formula for a feature \\( x \\) is given by:\n",
    "\n",
    "[x' = \\frac{x - \\text{min}(X)}{\\text{max}(X) - \\text{min}(X)}]\n",
    "\n",
    "Where:\n",
    "- \\( x' \\) is the scaled value,\n",
    "- \\( x \\) is the original value,\n",
    "- \\(\\text{min}(X)\\) is the minimum value of the feature,\n",
    "- \\(\\text{max}(X)\\) is the maximum value of the feature.\n",
    "\n",
    "This formula rescales the data so that:\n",
    "- The minimum value of the feature becomes 0,\n",
    "- The maximum value becomes 1,\n",
    "- All other values are proportionally adjusted.\n",
    "\n",
    "### Example of Min-Max Scaling\n",
    "\n",
    "Let's illustrate Min-Max scaling with a simple example:\n",
    "\n",
    "#### Original Dataset\n",
    "Assume we have a feature representing the ages of a group of individuals:\n",
    "\n",
    "| Age |\n",
    "|-----|\n",
    "| 22  |\n",
    "| 25  |\n",
    "| 30  |\n",
    "| 35  |\n",
    "| 40  |\n",
    "\n",
    "#### Step 1: Identify Minimum and Maximum\n",
    "- Minimum age (\\(\\text{min}(X)\\)): 22\n",
    "- Maximum age (\\(\\text{max}(X)\\)): 40\n",
    "\n",
    "#### Step 2: Apply Min-Max Scaling\n",
    "Now we can apply the Min-Max scaling formula to each age value:\n",
    "\n",
    "1. For Age 22:\n",
    "   \\[\n",
    "   x' = \\frac{22 - 22}{40 - 22} = 0\n",
    "   \\]\n",
    "   \n",
    "2. For Age 25:\n",
    "   \\[\n",
    "   x' = \\frac{25 - 22}{40 - 22} = \\frac{3}{18} \\approx 0.167\n",
    "   \\]\n",
    "\n",
    "3. For Age 30:\n",
    "   \\[\n",
    "   x' = \\frac{30 - 22}{40 - 22} = \\frac{8}{18} \\approx 0.444\n",
    "   \\]\n",
    "\n",
    "4. For Age 35:\n",
    "   \\[\n",
    "   x' = \\frac{35 - 22}{40 - 22} = \\frac{13}{18} \\approx 0.722\n",
    "   \\]\n",
    "\n",
    "5. For Age 40:\n",
    "   \\[\n",
    "   x' = \\frac{40 - 22}{40 - 22} = 1\n",
    "   \\]\n",
    "\n",
    "#### Scaled Dataset\n",
    "The resulting scaled ages are:\n",
    "\n",
    "| Age (Original) | Age (Scaled) |\n",
    "|----------------|---------------|\n",
    "| 22             | 0             |\n",
    "| 25             | 0.167         |\n",
    "| 30             | 0.444         |\n",
    "| 35             | 0.722         |\n",
    "| 40             | 1             |\n",
    "\n",
    "### Application of Min-Max Scaling\n",
    "\n",
    "Min-Max scaling is particularly beneficial in various machine learning algorithms:\n",
    "- **Neural Networks**: Helps in faster convergence during training since the features are on a similar scale.\n",
    "- **K-Nearest Neighbors**: Improves distance calculations, making sure that features contribute equally.\n",
    "- **Gradient Descent**: Leads to more efficient optimization as it avoids the issue of different scales affecting\n",
    "    the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cef478d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941842ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Unit Vector technique, also known as vector normalization, is a method of feature scaling that transforms\n",
    "features into unit vectors. This technique ensures that each feature vector has a length (or magnitude) of one,\n",
    "effectively scaling the values relative to their overall magnitude.\n",
    "\n",
    "### How the Unit Vector Technique Works\n",
    "\n",
    "The formula for scaling a feature vector \\( \\mathbf{x} \\) to a unit vector \\( \\mathbf{u} \\) is given by:\n",
    "\n",
    "\\[\n",
    "\\mathbf{u} = \\frac{\\mathbf{x}}{\\|\\mathbf{x}\\|} \n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( \\|\\mathbf{x}\\| \\) is the Euclidean norm (or length) of the vector, calculated as:\n",
    "  \n",
    "\\[\n",
    "\\|\\mathbf{x}\\| = \\sqrt{\\sum_{i=1}^{n} x_i^2}\n",
    "\\]\n",
    "\n",
    "This means that each component of the vector is divided by the vector’s length, resulting in a new vector with a\n",
    "length of 1.\n",
    "\n",
    "### Key Differences Between Unit Vector Technique and Min-Max Scaling\n",
    "\n",
    "1. **Output Range**:\n",
    "   - **Min-Max Scaling**: Transforms features to a fixed range, typically [0, 1].\n",
    "   - **Unit Vector Technique**: Transforms features into vectors with a magnitude of 1, maintaining the direction \n",
    "    but changing the scale.\n",
    "\n",
    "2. **Geometric Interpretation**:\n",
    "   - **Min-Max Scaling**: Rescales values based on the minimum and maximum, preserving the distribution within a\n",
    "    bounded interval.\n",
    "   - **Unit Vector Technique**: Focuses on the angle and direction of the data points, which can be useful in \n",
    "    applications like clustering or classification.\n",
    "\n",
    "3. **Application Context**:\n",
    "   - **Min-Max Scaling**: Commonly used when features have different ranges and need normalization for algorithms\n",
    "    sensitive to feature scale.\n",
    "   - **Unit Vector Technique**: Often used in contexts where the angle between feature vectors matters, such as in \n",
    "    text mining (e.g., cosine similarity).\n",
    "\n",
    "### Example of the Unit Vector Technique\n",
    "\n",
    "Let’s consider a simple example with a feature vector:\n",
    "\n",
    "#### Original Feature Vector\n",
    "Suppose we have a feature vector representing the scores of a student in different subjects:\n",
    "\n",
    "\\[\n",
    "\\mathbf{x} = [3, 4, 5]\n",
    "\\]\n",
    "\n",
    "#### Step 1: Calculate the Euclidean Norm\n",
    "First, we compute the Euclidean norm of the vector:\n",
    "\n",
    "\\[\n",
    "\\|\\mathbf{x}\\| = \\sqrt{3^2 + 4^2 + 5^2} = \\sqrt{9 + 16 + 25} = \\sqrt{50} \\approx 7.071\n",
    "\\]\n",
    "\n",
    "#### Step 2: Apply the Unit Vector Transformation\n",
    "Now, we scale the vector to make it a unit vector:\n",
    "\n",
    "\\[\n",
    "\\mathbf{u} = \\left[\\frac{3}{7.071}, \\frac{4}{7.071}, \\frac{5}{7.071}\\right] \\approx [0.424, 0.566, 0.707]\n",
    "\\]\n",
    "\n",
    "### Scaled Feature Vector\n",
    "The resulting unit vector is approximately:\n",
    "\n",
    "\\[\n",
    "\\mathbf{u} \\approx [0.424, 0.566, 0.707]\n",
    "\\]\n",
    "\n",
    "### Application of the Unit Vector Technique\n",
    "\n",
    "The Unit Vector technique is particularly useful in applications involving:\n",
    "- **Text Analysis**: Converting documents to vector representations where cosine similarity (based on angles) is\n",
    "    used for measuring similarity.\n",
    "- **Clustering Algorithms**: Such as k-means, where the orientation of data points is more critical than their \n",
    "    absolute distances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2921f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9fed5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Principal Component Analysis (PCA) is a statistical technique used for dimensionality reduction while preserving as\n",
    "much variance (information) as possible in the data. PCA transforms a set of correlated variables into a set of \n",
    "uncorrelated variables called principal components, which are ordered by the amount of variance they capture.\n",
    "\n",
    "### How PCA Works\n",
    "\n",
    "1. **Standardization**: If the features have different scales, standardize the data to have a mean of zero and a \n",
    "    standard deviation of one. This is important to ensure that PCA is not biased towards variables with larger scales.\n",
    "\n",
    "2. **Covariance Matrix Computation**: Calculate the covariance matrix of the standardized data to understand how the\n",
    "    variables vary together.\n",
    "\n",
    "3. **Eigenvalues and Eigenvectors**: Compute the eigenvalues and eigenvectors of the covariance matrix. The eigenvalues\n",
    "    indicate the amount of variance captured by each principal component, while the eigenvectors provide the direction\n",
    "    of these components.\n",
    "\n",
    "4. **Sort and Select Components**: Sort the eigenvalues in descending order and select the top \\( k \\) eigenvectors \n",
    "    (principal components) that correspond to the largest eigenvalues. These components capture the most variance in\n",
    "    the data.\n",
    "\n",
    "5. **Transform the Data**: Project the original data onto the selected principal components, resulting in a new \n",
    "    dataset with reduced dimensions.\n",
    "\n",
    "### Example of PCA Application\n",
    "\n",
    "Let's illustrate PCA with a simple example using a dataset of 2D points.\n",
    "\n",
    "#### Original Dataset\n",
    "Suppose we have the following 2D data points representing the scores of students in two subjects:\n",
    "\n",
    "| Subject 1 | Subject 2 |\n",
    "|------------|------------|\n",
    "| 2          | 3          |\n",
    "| 3          | 5          |\n",
    "| 4          | 6          |\n",
    "| 5          | 8          |\n",
    "| 6          | 10         |\n",
    "\n",
    "#### Step 1: Standardization\n",
    "First, we standardize the data (subtract the mean and divide by the standard deviation).\n",
    "\n",
    "Assume the mean values are:\n",
    "- Mean of Subject 1 = 4\n",
    "- Mean of Subject 2 = 6\n",
    "\n",
    "After standardization, the data might look like this:\n",
    "\n",
    "| Subject 1 (Standardized) | Subject 2 (Standardized) |\n",
    "|---------------------------|---------------------------|\n",
    "| -1.264                    | -1.264                    |\n",
    "| -0.632                    | -0.632                    |\n",
    "| 0                         | 0                         |\n",
    "| 0.632                    | 0.632                     |\n",
    "| 1.264                    | 1.264                     |\n",
    "\n",
    "#### Step 2: Covariance Matrix\n",
    "Next, compute the covariance matrix of the standardized data. For our standardized data, the covariance matrix \n",
    "could be:\n",
    "\n",
    "\\[\n",
    "\\begin{bmatrix}\n",
    "1 & 0.95 \\\\\n",
    "0.95 & 1\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "#### Step 3: Eigenvalues and Eigenvectors\n",
    "Calculate the eigenvalues and eigenvectors of the covariance matrix. Let's assume we find:\n",
    "\n",
    "- Eigenvalues: \\( \\lambda_1 = 1.9 \\), \\( \\lambda_2 = 0.1 \\)\n",
    "- Eigenvectors: \n",
    "  - \\( \\mathbf{e}_1 = [0.707, 0.707] \\)\n",
    "  - \\( \\mathbf{e}_2 = [-0.707, 0.707] \\)\n",
    "\n",
    "#### Step 4: Sort and Select Components\n",
    "Sort the eigenvalues and select the top \\( k \\) components. Here, we would choose the first principal component \n",
    "corresponding to \\( \\lambda_1 = 1.9 \\) since it captures the majority of the variance.\n",
    "\n",
    "#### Step 5: Transform the Data\n",
    "Finally, project the original standardized data onto the selected principal component:\n",
    "\n",
    "\\[\n",
    "\\mathbf{P} = \\mathbf{X} \\cdot \\mathbf{e}_1\n",
    "\\]\n",
    "\n",
    "Where \\( \\mathbf{X} \\) is the matrix of standardized data.\n",
    "\n",
    "The transformed data will have reduced dimensions (from 2D to 1D), effectively summarizing the original data's \n",
    "variance in a single dimension.\n",
    "\n",
    "### Application of PCA\n",
    "\n",
    "PCA is commonly used in various fields, including:\n",
    "- **Data Visualization**: Reducing the dimensions of complex datasets for visualization in 2D or 3D.\n",
    "- **Noise Reduction**: Eliminating less informative components that capture noise rather than signal.\n",
    "- **Preprocessing for Machine Learning**: Simplifying datasets before applying machine learning algorithms to \n",
    "    improve performance and reduce overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff7d0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aa72ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Principal Component Analysis (PCA) is closely related to feature extraction, as it is a method used to derive new\n",
    "features from a dataset. Feature extraction involves transforming the original set of features into a new set of \n",
    "features that better represent the underlying structure of the data, often with the goal of reducing dimensionality \n",
    "while retaining significant information.\n",
    "\n",
    "### Relationship Between PCA and Feature Extraction\n",
    "\n",
    "1. **Dimensionality Reduction**: PCA reduces the number of features (dimensions) by transforming the original features\n",
    "    into a smaller set of principal components that capture the most variance in the data. This is a form of feature \n",
    "    extraction because the new components are derived from the original features.\n",
    "\n",
    "2. **New Feature Representation**: The principal components created by PCA are linear combinations of the original \n",
    "    features. This means that PCA can create new features that highlight the relationships and variations in the data\n",
    "    more effectively than the original features.\n",
    "\n",
    "3. **Information Retention**: PCA aims to retain as much information as possible in fewer dimensions. By focusing on \n",
    "    the components with the highest variance, PCA emphasizes the most informative aspects of the data, making it a \n",
    "    useful technique for feature extraction.\n",
    "\n",
    "### How PCA Can Be Used for Feature Extraction\n",
    "\n",
    "1. **Data Preparation**: Start with a dataset and preprocess it (standardize if necessary).\n",
    "2. **Apply PCA**: Calculate the covariance matrix, extract eigenvalues and eigenvectors, and select the top \\( k \\) \n",
    "    principal components.\n",
    "3. **Transform the Data**: Project the original dataset onto the selected principal components to create a new feature\n",
    "    set.\n",
    "\n",
    "### Example of PCA for Feature Extraction\n",
    "\n",
    "#### Original Dataset\n",
    "Consider a dataset with three features representing different aspects of houses:\n",
    "\n",
    "| Size (sq ft) | Bedrooms | Age (years) |\n",
    "|--------------|----------|-------------|\n",
    "| 1500         | 3        | 10          |\n",
    "| 1600         | 3        | 15          |\n",
    "| 1700         | 4        | 20          |\n",
    "| 1800         | 4        | 25          |\n",
    "| 1900         | 5        | 30          |\n",
    "\n",
    "#### Step 1: Standardization\n",
    "Standardize the dataset to have mean 0 and standard deviation 1.\n",
    "\n",
    "#### Step 2: Compute the Covariance Matrix\n",
    "Calculate the covariance matrix of the standardized features.\n",
    "\n",
    "#### Step 3: Eigenvalues and Eigenvectors\n",
    "Determine the eigenvalues and eigenvectors of the covariance matrix.\n",
    "\n",
    "#### Step 4: Select Principal Components\n",
    "Assuming the eigenvalues are:\n",
    "\n",
    "- \\( \\lambda_1 = 2.5 \\) (first component)\n",
    "- \\( \\lambda_2 = 0.8 \\) (second component)\n",
    "- \\( \\lambda_3 = 0.1 \\) (third component)\n",
    "\n",
    "We might choose the first two principal components since they capture the most variance.\n",
    "\n",
    "#### Step 5: Transform the Data\n",
    "Project the original standardized data onto the selected principal components. The new feature set might look like \n",
    "this:\n",
    "\n",
    "| PC1   | PC2   |\n",
    "|-------|-------|\n",
    "| 1.5   | -0.2  |\n",
    "| 1.7   | -0.1  |\n",
    "| 1.8   | 0.0   |\n",
    "| 1.9   | 0.1   |\n",
    "| 2.0   | 0.2   |\n",
    "\n",
    "### Application of the Extracted Features\n",
    "\n",
    "In this example, the original three features (Size, Bedrooms, Age) have been transformed into two new features \n",
    "(PC1 and PC2). These new features can be used in further analysis, such as building a regression model to predict\n",
    "house prices or for clustering similar houses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c64d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10653e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "To preprocess the dataset for a recommendation system for a food delivery service using Min-Max scaling, we would\n",
    "follow a structured approach to ensure that features such as price, rating, and delivery time are appropriately \n",
    "scaled. Here’s how you can implement Min-Max scaling in this context:\n",
    "\n",
    "### Step-by-Step Approach\n",
    "\n",
    "#### Step 1: Understand the Dataset\n",
    "\n",
    "First, familiarize yourself with the dataset, which may contain features like:\n",
    "\n",
    "- **Price**: The cost of the food items.\n",
    "- **Rating**: User ratings (e.g., from 1 to 5).\n",
    "- **Delivery Time**: The time taken for delivery (in minutes).\n",
    "\n",
    "#### Step 2: Explore and Clean the Data\n",
    "\n",
    "- **Check for Missing Values**: Identify any missing values in the dataset and decide how to handle them \n",
    "    (imputation or removal).\n",
    "- **Inspect Data Types**: Ensure that the features are of appropriate types (e.g., numeric for price, rating, \n",
    "    and delivery time).\n",
    "\n",
    "#### Step 3: Apply Min-Max Scaling\n",
    "\n",
    "Min-Max scaling will transform each feature to a range of [0, 1]. Here’s how to do it:\n",
    "\n",
    "1. **Identify the Features**: Choose the features to scale (price, rating, and delivery time).\n",
    "\n",
    "2. **Calculate Min and Max**: For each feature, calculate the minimum and maximum values from the dataset.\n",
    "\n",
    "3. **Apply the Min-Max Scaling Formula**:\n",
    "\n",
    "   The formula for scaling a feature \\( x \\) to \\( x' \\) is:\n",
    "\n",
    "   \\[\n",
    "   x' = \\frac{x - \\text{min}(X)}{\\text{max}(X) - \\text{min}(X)}\n",
    "   \\]\n",
    "\n",
    "   where:\n",
    "   - \\( x' \\) is the scaled value,\n",
    "   - \\( \\text{min}(X) \\) and \\( \\text{max}(X) \\) are the minimum and maximum values of the feature.\n",
    "\n",
    "4. **Transform Each Feature**: For each record in the dataset, apply the scaling formula for each feature.\n",
    "\n",
    "#### Example of Min-Max Scaling\n",
    "\n",
    "Let’s assume the following values in your dataset:\n",
    "\n",
    "| Price | Rating | Delivery Time |\n",
    "|-------|--------|---------------|\n",
    "| 10    | 4.5    | 30            |\n",
    "| 15    | 3.8    | 25            |\n",
    "| 20    | 4.0    | 40            |\n",
    "| 25    | 5.0    | 20            |\n",
    "| 30    | 4.2    | 35            |\n",
    "\n",
    "**Calculating Min and Max**:\n",
    "- **Price**: Min = 10, Max = 30\n",
    "- **Rating**: Min = 3.8, Max = 5.0\n",
    "- **Delivery Time**: Min = 20, Max = 40\n",
    "\n",
    "**Applying Min-Max Scaling**:\n",
    "\n",
    "1. **Price**:\n",
    "   - For \\( \\text{Price} = 10 \\):\n",
    "     \\[\n",
    "     \\text{Price}' = \\frac{10 - 10}{30 - 10} = 0\n",
    "     \\]\n",
    "   - For \\( \\text{Price} = 15 \\):\n",
    "     \\[\n",
    "     \\text{Price}' = \\frac{15 - 10}{30 - 10} = 0.25\n",
    "     \\]\n",
    "   - Continue for other prices...\n",
    "\n",
    "2. **Rating**:\n",
    "   - For \\( \\text{Rating} = 4.5 \\):\n",
    "     \\[\n",
    "     \\text{Rating}' = \\frac{4.5 - 3.8}{5.0 - 3.8} \\approx 0.5\n",
    "     \\]\n",
    "   - Continue for other ratings...\n",
    "\n",
    "3. **Delivery Time**:\n",
    "   - For \\( \\text{Delivery Time} = 30 \\):\n",
    "     \\[\n",
    "     \\text{Delivery Time}' = \\frac{30 - 20}{40 - 20} = 0.5\n",
    "     \\]\n",
    "   - Continue for other delivery times...\n",
    "\n",
    "#### Step 4: Create the Scaled Dataset\n",
    "\n",
    "After applying Min-Max scaling, your transformed dataset will look something like this:\n",
    "\n",
    "| Price' | Rating' | Delivery Time' |\n",
    "|--------|---------|-----------------|\n",
    "| 0      | 0.5     | 0.5             |\n",
    "| 0.25   | 0      | 0.25            |\n",
    "| 0.5    | 0.25    | 1               |\n",
    "| 0.75   | 1       | 0               |\n",
    "| 1      | 0.75    | 0.75            |\n",
    "\n",
    "### Step 5: Use the Scaled Features for Modeling\n",
    "\n",
    "Now that the features are scaled to a common range, they can be used effectively in building the recommendation \n",
    "    system. Scaling ensures that no single feature disproportionately influences the model, which is particularly\n",
    "    important for algorithms like collaborative filtering or any model relying on distance metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd4d7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd5a33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "To build a model for predicting stock prices using PCA (Principal Component Analysis) for dimensionality reduction, \n",
    "you would follow these steps:\n",
    "\n",
    "### Step-by-Step Approach to Using PCA\n",
    "\n",
    "#### Step 1: Understand the Dataset\n",
    "\n",
    "Begin by examining the dataset, which might include various features such as:\n",
    "\n",
    "- **Company Financial Data**: Earnings per share, revenue, debt levels, etc.\n",
    "- **Market Trends**: Historical prices, trading volume, market indices, etc.\n",
    "- **Other Indicators**: Economic indicators, interest rates, etc.\n",
    "\n",
    "#### Step 2: Data Preprocessing\n",
    "\n",
    "1. **Handling Missing Values**: Identify and address any missing values in the dataset. You might use imputation \n",
    "    or remove rows/columns with excessive missing data.\n",
    "\n",
    "2. **Standardization**: Since PCA is sensitive to the scales of the features, standardize the data so that each \n",
    "    feature has a mean of 0 and a standard deviation of 1. This can be done using the formula:\n",
    "\n",
    "   \\[\n",
    "   z = \\frac{x - \\mu}{\\sigma}\n",
    "   \\]\n",
    "\n",
    "   where \\( \\mu \\) is the mean and \\( \\sigma \\) is the standard deviation of the feature.\n",
    "\n",
    "#### Step 3: Apply PCA\n",
    "\n",
    "1. **Calculate the Covariance Matrix**: After standardization, compute the covariance matrix to understand how the\n",
    "    features relate to one another.\n",
    "\n",
    "2. **Eigenvalues and Eigenvectors**: Compute the eigenvalues and eigenvectors of the covariance matrix. \n",
    "    The eigenvalues represent the amount of variance captured by each principal component.\n",
    "\n",
    "3. **Sort Eigenvalues**: Sort the eigenvalues in descending order. The corresponding eigenvectors indicate the\n",
    "    directions of the new feature space.\n",
    "\n",
    "4. **Select Principal Components**: Choose the top \\( k \\) principal components based on the eigenvalues.\n",
    "    You can decide the number \\( k \\) based on the cumulative explained variance. For example, you might want \n",
    "    to retain 95% of the variance in the dataset.\n",
    "\n",
    "#### Step 4: Transform the Data\n",
    "\n",
    "1. **Project the Data**: Transform the original standardized dataset onto the selected principal components to \n",
    "    create a new dataset with reduced dimensions. This can be expressed as:\n",
    "\n",
    "   \\[\n",
    "   Z = X \\cdot W\n",
    "   \\]\n",
    "\n",
    "   where \\( Z \\) is the transformed dataset, \\( X \\) is the standardized original dataset, and \\( W \\) is the \n",
    "    matrix of selected eigenvectors.\n",
    "\n",
    "#### Step 5: Use the Reduced Dataset for Modeling\n",
    "\n",
    "Now that you have a dataset with reduced dimensions (fewer features), you can use this transformed data to build\n",
    "your stock price prediction model. The reduced set of features captures the most important variance in the data \n",
    "while eliminating noise and redundancy.\n",
    "\n",
    "### Example of PCA Application\n",
    "\n",
    "Suppose you have a dataset with the following features:\n",
    "\n",
    "| Earnings per Share | Revenue | Debt | Market Trend Score | Trading Volume |\n",
    "|---------------------|---------|------|--------------------|----------------|\n",
    "| 1.2                 | 5000    | 100  | 0.5                | 200000         |\n",
    "| 1.5                 | 6000    | 150  | 0.6                | 250000         |\n",
    "| 1.8                 | 5500    | 120  | 0.7                | 180000         |\n",
    "| ...                 | ...     | ...  | ...                | ...            |\n",
    "\n",
    "1. **Standardize** the features to have a mean of 0 and standard deviation of 1.\n",
    "\n",
    "2. **Calculate the covariance matrix**, then find eigenvalues and eigenvectors.\n",
    "\n",
    "3. **Sort the eigenvalues** and determine the number of principal components to keep (e.g., 2 or 3).\n",
    "\n",
    "4. **Transform the dataset** to reduce it to the selected number of principal components.\n",
    "\n",
    "### Step 6: Evaluate and Interpret Results\n",
    "\n",
    "After building your predictive model using the reduced dataset, assess its performance using appropriate metrics \n",
    "(e.g., RMSE, MAE). You can also interpret the importance of the principal components in relation to the original \n",
    "features to gain insights into the factors driving stock prices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859724f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5004ec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "To perform Min-Max scaling to transform the values of a dataset to a range of \\([-1, 1]\\), we can use the following\n",
    "formula:\n",
    "\n",
    "\\[\n",
    "x' = 2 \\cdot \\frac{x - \\text{min}(X)}{\\text{max}(X) - \\text{min}(X)} - 1\n",
    "\\]\n",
    "\n",
    "where:\n",
    "- \\(x'\\) is the scaled value,\n",
    "- \\(x\\) is the original value,\n",
    "- \\(\\text{min}(X)\\) is the minimum value in the dataset,\n",
    "- \\(\\text{max}(X)\\) is the maximum value in the dataset.\n",
    "\n",
    "### Given Dataset\n",
    "\\[ \\text{Values} = [1, 5, 10, 15, 20] \\]\n",
    "\n",
    "### Step 1: Find the Minimum and Maximum Values\n",
    "\n",
    "- \\(\\text{min}(X) = 1\\)\n",
    "- \\(\\text{max}(X) = 20\\)\n",
    "\n",
    "### Step 2: Apply the Scaling Formula\n",
    "\n",
    "Now, we will apply the Min-Max scaling formula to each value in the dataset.\n",
    "\n",
    "1. For \\(x = 1\\):\n",
    "   \\[\n",
    "   x' = 2 \\cdot \\frac{1 - 1}{20 - 1} - 1 = 2 \\cdot \\frac{0}{19} - 1 = -1\n",
    "   \\]\n",
    "\n",
    "2. For \\(x = 5\\):\n",
    "   \\[\n",
    "   x' = 2 \\cdot \\frac{5 - 1}{20 - 1} - 1 = 2 \\cdot \\frac{4}{19} - 1 \\approx 0.4211\n",
    "   \\]\n",
    "\n",
    "3. For \\(x = 10\\):\n",
    "   \\[\n",
    "   x' = 2 \\cdot \\frac{10 - 1}{20 - 1} - 1 = 2 \\cdot \\frac{9}{19} - 1 \\approx -0.0526\n",
    "   \\]\n",
    "\n",
    "4. For \\(x = 15\\):\n",
    "   \\[\n",
    "   x' = 2 \\cdot \\frac{15 - 1}{20 - 1} - 1 = 2 \\cdot \\frac{14}{19} - 1 \\approx 0.4737\n",
    "   \\]\n",
    "\n",
    "5. For \\(x = 20\\):\n",
    "   \\[\n",
    "   x' = 2 \\cdot \\frac{20 - 1}{20 - 1} - 1 = 2 \\cdot \\frac{19}{19} - 1 = 1\n",
    "   \\]\n",
    "\n",
    "### Step 3: Summary of Transformed Values\n",
    "\n",
    "After applying the Min-Max scaling to the range of \\([-1, 1]\\), the transformed values are:\n",
    "\n",
    "- For 1: \\( -1 \\)\n",
    "- For 5: \\( \\approx 0.4211 \\)\n",
    "- For 10: \\( \\approx -0.0526 \\)\n",
    "- For 15: \\( \\approx 0.4737 \\)\n",
    "- For 20: \\( 1 \\)\n",
    "\n",
    "### Final Transformed Dataset\n",
    "The final scaled dataset is approximately:\n",
    "\n",
    "\\[\n",
    "[-1, 0.4211, -0.0526, 0.4737, 1]\n",
    "\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483ecf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61667902",
   "metadata": {},
   "outputs": [],
   "source": [
    "To perform feature extraction using PCA (Principal Component Analysis) on a dataset containing features such as height,\n",
    "weight, age, gender, and blood pressure, we would follow a structured approach. Here's how to determine how many \n",
    "principal components to retain and the reasoning behind the choice.\n",
    "\n",
    "### Step-by-Step Approach to PCA\n",
    "\n",
    "#### Step 1: Prepare the Dataset\n",
    "\n",
    "1. **Data Collection**: Ensure your dataset contains numerical representations for all features. Since \"gender\" is\n",
    "    categorical, it needs to be encoded (e.g., using one-hot encoding).\n",
    "2. **Standardization**: Standardize the numerical features (height, weight, age, blood pressure) to have a mean of\n",
    "    0 and a standard deviation of 1. This step is essential as PCA is sensitive to the scale of the features.\n",
    "\n",
    "#### Step 2: Apply PCA\n",
    "\n",
    "1. **Calculate Covariance Matrix**: Compute the covariance matrix of the standardized data to understand how the \n",
    "    features relate to each other.\n",
    "2. **Eigenvalues and Eigenvectors**: Calculate the eigenvalues and eigenvectors of the covariance matrix.\n",
    "3. **Sort Eigenvalues**: Sort the eigenvalues in descending order to identify which components explain the most \n",
    "    variance.\n",
    "\n",
    "#### Step 3: Determine the Number of Principal Components to Retain\n",
    "\n",
    "1. **Cumulative Explained Variance**: Create a plot (scree plot) of the cumulative explained variance by each \n",
    "    principal component. The explained variance for each principal component indicates how much variance in the \n",
    "    dataset is captured by that component.\n",
    "2. **Choosing \\(k\\)**: Decide on the number of principal components to retain based on the cumulative explained \n",
    "    variance. A common criterion is to retain enough components to capture a certain percentage of the total \n",
    "    variance, such as 95% or 90%. \n",
    "\n",
    "   - **Kaiser Criterion**: You might also consider retaining components with eigenvalues greater than 1, as they\n",
    "    explain more variance than a single original feature.\n",
    "\n",
    "3. **Visual Inspection**: In addition to the cumulative explained variance, visually inspecting the scree plot can \n",
    "    help identify an \"elbow point,\" where the addition of more components provides diminishing returns in variance\n",
    "    explained.\n",
    "\n",
    "### Example Decision\n",
    "\n",
    "Assuming after performing PCA, you find the following explained variance:\n",
    "\n",
    "- **PC1**: 40%\n",
    "- **PC2**: 30%\n",
    "- **PC3**: 20%\n",
    "- **PC4**: 5%\n",
    "- **PC5**: 5%\n",
    "\n",
    "The cumulative explained variance would be:\n",
    "\n",
    "- PC1: 40%\n",
    "- PC2: 70%\n",
    "- PC3: 90%\n",
    "- PC4: 95%\n",
    "- PC5: 100%\n",
    "\n",
    "### Choosing Principal Components\n",
    "\n",
    "In this example, if you aim to retain 90% of the variance, you would choose:\n",
    "\n",
    "- **3 Principal Components (PC1, PC2, PC3)**: These three components capture 90% of the variance in the data, \n",
    "    providing a good balance between reducing dimensionality and retaining significant information.\n",
    "\n",
    "If you were to retain 95% of the variance, you might opt for:\n",
    "\n",
    "- **4 Principal Components (PC1, PC2, PC3, PC4)**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
