{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63e2fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Explain the difference between linear regression and logistic regression models. Provide an example of\n",
    "a scenario where logistic regression would be more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aed462",
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear regression and logistic regression are both statistical methods used for modeling relationships between \n",
    "variables, but they are used for different types of outcomes and have distinct characteristics. Hereâ€™s a comparison\n",
    "of the two:\n",
    "\n",
    "### 1. **Purpose**:\n",
    "   - **Linear Regression**:\n",
    "     - Used to predict a continuous outcome (dependent variable) based on one or more independent variables.\n",
    "     - The relationship is modeled as a linear equation (e.g., \\( y = mx + b \\)).\n",
    "   - **Logistic Regression**:\n",
    "     - Used to predict a binary outcome (dependent variable) based on one or more independent variables.\n",
    "     - It models the probability that a given input point belongs to a certain class (typically coded as 0 or 1) \n",
    "    using the logistic function.\n",
    "\n",
    "### 2. **Output**:\n",
    "   - **Linear Regression**:\n",
    "     - Produces a continuous output, which can take any real number.\n",
    "   - **Logistic Regression**:\n",
    "     - Produces an output that represents probabilities between 0 and 1, which can then be converted into binary \n",
    "        classes using a threshold (commonly 0.5).\n",
    "\n",
    "### 3. **Mathematical Function**:\n",
    "   - **Linear Regression**:\n",
    "     - The formula is linear: \\( y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n + \\epsilon \\).\n",
    "   - **Logistic Regression**:\n",
    "     - The output is transformed using the logistic function: \n",
    "       [P(y=1|X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n)}}]\n",
    "     - This produces a probability that can be used for classification.\n",
    "\n",
    "### 4. **Assumptions**:\n",
    "   - **Linear Regression**:\n",
    "     - Assumes linearity between the dependent and independent variables, normally distributed errors, and \n",
    "    homoscedasticity (constant variance of errors).\n",
    "   - **Logistic Regression**:\n",
    "     - Does not assume a linear relationship between the dependent and independent variables but assumes that the \n",
    "        log-odds of the outcome are linearly related to the independent variables.\n",
    "\n",
    "### Example Scenario for Logistic Regression:\n",
    "**Scenario**: Predicting whether a customer will purchase a product (Yes/No).\n",
    "\n",
    "- **Context**: A retail company wants to understand whether various factors (like age, income, and past purchase \n",
    "behavior) influence whether a customer makes a purchase.\n",
    "- **Appropriateness**: In this case, logistic regression is more appropriate because the outcome is binary\n",
    "(purchase or no purchase). The model can estimate the probability of a customer making a purchase based on the input \n",
    "features, allowing the company to identify high-risk and low-risk customers effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7372a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is the cost function used in logistic regression, and how is it optimized?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac9b251",
   "metadata": {},
   "outputs": [],
   "source": [
    "In logistic regression, the cost function used is the **logistic loss function** (also known as binary cross-entropy \n",
    "loss). This function measures how well the predicted probabilities of the model match the actual binary outcomes \n",
    "(0 or 1). Here's a breakdown of the cost function and how it is optimized:\n",
    "\n",
    "### Cost Function\n",
    "\n",
    "For logistic regression, the cost function \\( J(\\theta) \\) for a dataset with \\( m \\) training examples can be \n",
    "expressed as:\n",
    "\n",
    "[J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)})) \\right]]\n",
    "\n",
    "Where:\n",
    "- ( y^{(i)} \\) is the actual label (0 or 1) for the \\( i \\)-th training example.\n",
    "- ( h_\\theta(x^{(i)}) \\) is the predicted probability that \\( y^{(i)} = 1 \\), given by the logistic function:\n",
    "  \n",
    "[h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^T x}}]\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- The first term, \\( y^{(i)} \\log(h_\\theta(x^{(i)})) \\), contributes to the cost when the actual label is 1.\n",
    "- The second term, \\( (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)})) \\), contributes to the cost when the actual label is 0.\n",
    "- The overall goal is to minimize this cost function so that the model's predictions are as close as possible to the \n",
    "actual outcomes.\n",
    "\n",
    "### Optimization\n",
    "\n",
    "To optimize the cost function and find the best parameters \\( \\theta \\), several methods can be used:\n",
    "\n",
    "1. **Gradient Descent**:\n",
    "   - **Basic Idea**: Iteratively update the parameters \\( \\theta \\) in the direction of the negative gradient of the \n",
    "    cost function.\n",
    "   - **Update Rule**:\n",
    "     [theta := \\theta - \\alpha \\nabla J(\\theta)]\n",
    "     Where \\( \\alpha \\) is the learning rate and \\( \\nabla J(\\theta) \\) is the gradient of the cost function with \n",
    "    respect to \\( \\theta \\).\n",
    "   - **Batch Gradient Descent**: Uses the entire dataset to compute the gradient in each iteration.\n",
    "   - **Stochastic Gradient Descent (SGD)**: Uses one training example at a time to compute the gradient, which can \n",
    "    lead to faster convergence.\n",
    "\n",
    "2. **Newton's Method**:\n",
    "   - Also known as the **Newton-Raphson method**, this is a second-order optimization method that uses the Hessian\n",
    "    matrix (the matrix of second derivatives) to update the parameters. It can converge faster than gradient descent,\n",
    "    especially near the optimum.\n",
    "\n",
    "3. **Other Optimization Algorithms**:\n",
    "   - Algorithms such as **L-BFGS**, **Adam**, or **RMSprop** can also be used, especially in larger datasets or when \n",
    "using regularization technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9d1597",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. Explain the concept of regularization in logistic regression and how it helps prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16113e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regularization in logistic regression is a technique used to prevent overfitting by adding a penalty to the cost \n",
    "function. Overfitting occurs when the model learns not only the underlying patterns in the training data but also \n",
    "the noise, leading to poor generalization on unseen data. Regularization helps to constrain the complexity of the \n",
    "model, making it simpler and more robust.\n",
    "\n",
    "### Types of Regularization\n",
    "\n",
    "1. **L1 Regularization (Lasso)**:\n",
    "   - Adds the absolute values of the coefficients as a penalty to the cost function.\n",
    "   - The modified cost function for logistic regression with L1 regularization is:\n",
    "     [J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)})) \\right] + \\lambda \\sum_{j=1}^{n} |\\theta_j|]\n",
    "   - Here, \\( \\lambda \\) is the regularization parameter that controls the strength of the penalty.\n",
    "   - L1 regularization can lead to sparse solutions, effectively setting some coefficients to zero, which aids in \n",
    "    feature selection.\n",
    "\n",
    "2. **L2 Regularization (Ridge)**:\n",
    "   - Adds the square of the coefficients as a penalty to the cost function.\n",
    "   - The modified cost function for logistic regression with L2 regularization is:\n",
    "     [J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)})) \\right] + \\lambda \\sum_{j=1}^{n} \\theta_j^2]\n",
    "   - L2 regularization discourages large coefficients but does not set them exactly to zero. This tends to retain all \n",
    "features while shrinking their impact, leading to a more generalized model.\n",
    "\n",
    "### How Regularization Helps Prevent Overfitting\n",
    "\n",
    "1. **Constrained Complexity**: By adding a penalty term to the cost function, regularization effectively constrains the\n",
    "    size of the coefficients. This discourages the model from fitting the training data too closely.\n",
    "\n",
    "2. **Smoother Decision Boundaries**: Regularization helps to create smoother decision boundaries, making the model less\n",
    "    sensitive to fluctuations in the training data. This can lead to better generalization on new data.\n",
    "\n",
    "3. **Feature Selection**: In the case of L1 regularization, the ability to set some coefficients to zero can help in \n",
    "    selecting only the most relevant features, reducing noise and enhancing model interpretability.\n",
    "\n",
    "4. **Bias-Variance Tradeoff**: Regularization introduces a bias into the model by simplifying it, which can help reduce\n",
    "    variance (the model's sensitivity to small fluctuations in the training set). A balanced bias-variance tradeoff is crucial for effective model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148eda8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is the ROC curve, and how is it used to evaluate the performance of the logistic regression model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766def85",
   "metadata": {},
   "outputs": [],
   "source": [
    "The ROC (Receiver Operating Characteristic) curve is a graphical representation used to evaluate the performance of a\n",
    "binary classification model, such as logistic regression. It illustrates the trade-off between sensitivity \n",
    "(true positive rate) and specificity (1 - false positive rate) across different threshold values. Hereâ€™s a detailed\n",
    "explanation of the ROC curve and how it is used to assess model performance:\n",
    "\n",
    "### Key Components of the ROC Curve\n",
    "\n",
    "1. **True Positive Rate (TPR)**:\n",
    "   - Also known as sensitivity or recall, it measures the proportion of actual positives that are correctly identified \n",
    "by the model.\n",
    "   - Formula: \n",
    "     [text{TPR} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}} = \\frac{TP}{TP + FN}]\n",
    "\n",
    "2. **False Positive Rate (FPR)**:\n",
    "   - Measures the proportion of actual negatives that are incorrectly identified as positives by the model.\n",
    "   - Formula:\n",
    "     [text{FPR} = \\frac{\\text{False Positives}}{\\text{False Positives} + \\text{True Negatives}} = \\frac{FP}{FP + TN}]\n",
    "\n",
    "### Constructing the ROC Curve\n",
    "\n",
    "1. **Threshold Variation**:\n",
    "   - The ROC curve is generated by varying the decision threshold for classifying an instance as positive. For each \n",
    "threshold, calculate TPR and FPR.\n",
    "   - As the threshold decreases, TPR generally increases while FPR also increases, leading to the characteristic curve.\n",
    "\n",
    "2. **Plotting the Curve**:\n",
    "   - The ROC curve is plotted with the FPR on the x-axis and TPR on the y-axis.\n",
    "   - The curve typically starts at (0, 0) and ends at (1, 1), representing the worst and best possible classifiers, \n",
    "    respectively.\n",
    "\n",
    "### Evaluating Model Performance\n",
    "\n",
    "1. **Area Under the ROC Curve (AUC)**:\n",
    "   - The performance of the model is often summarized using the Area Under the ROC Curve (AUC).\n",
    "   - AUC values range from 0 to 1:\n",
    "     - **0.5**: Indicates no discriminative ability (equivalent to random guessing).\n",
    "     - **1.0**: Indicates perfect discrimination between classes.\n",
    "     - **0.7 - 0.8**: Indicates acceptable performance.\n",
    "     - **0.8 - 0.9**: Indicates excellent performance.\n",
    "     - **> 0.9**: Indicates outstanding performance.\n",
    "\n",
    "2. **Threshold Selection**:\n",
    "   - The ROC curve helps identify an optimal threshold based on the desired balance between sensitivity and specificity.\n",
    "By analyzing the curve, you can select a point that maximizes TPR while minimizing FPR based on the specific \n",
    "application requirements.\n",
    "\n",
    "3. **Comparative Analysis**:\n",
    "   - ROC curves can be compared across different models to evaluate which model performs better at distinguishing \n",
    "between the positive and negative classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1c8a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What are some common techniques for feature selection in logistic regression? How do these\n",
    "techniques help improve the model's performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cb4acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature selection is an essential step in the modeling process, particularly for logistic regression, as it helps \n",
    "improve model performance by reducing complexity, enhancing interpretability, and mitigating the risk of overfitting.\n",
    "Here are some common techniques for feature selection in logistic regression:\n",
    "\n",
    "### 1. **Filter Methods**:\n",
    "   - **Statistical Tests**: Use statistical measures (e.g., chi-square test, ANOVA, or correlation coefficients) to \n",
    "        evaluate the relationship between each feature and the target variable. Features with significant relationships\n",
    "        are retained, while others are discarded.\n",
    "   - **Variance Threshold**: Remove features with low variance, as they contribute little to the model's predictive\n",
    "    power.\n",
    "\n",
    "### 2. **Wrapper Methods**:\n",
    "   - **Recursive Feature Elimination (RFE)**: This technique recursively removes the least important features based on\n",
    "        the modelâ€™s performance until the optimal number of features is reached. It evaluates subsets of features by\n",
    "        training the model multiple times.\n",
    "   - **Forward Selection**: Start with no features and add them one by one based on which feature improves the model\n",
    "    performance the most until no significant improvements are observed.\n",
    "   - **Backward Elimination**: Start with all features and remove them one by one based on the least significant \n",
    "    feature until the model performance deteriorates.\n",
    "\n",
    "### 3. **Embedded Methods**:\n",
    "   - **Regularization Techniques**:\n",
    "     - **L1 Regularization (Lasso)**: Encourages sparsity by adding a penalty to the absolute size of the coefficients,\n",
    "        effectively setting some coefficients to zero. This naturally selects important features while discarding \n",
    "        others.\n",
    "     - **L2 Regularization (Ridge)**: While it does not set coefficients to zero, it shrinks their values, reducing \n",
    "        the influence of less important features.\n",
    "\n",
    "### 4. **Model-Based Feature Importance**:\n",
    "   - **Logistic Regression Coefficients**: Analyze the magnitude of the coefficients after fitting the logistic\n",
    "        regression model. Features with larger absolute coefficients are generally more influential in predicting\n",
    "        the outcome.\n",
    "   - **Tree-based Methods**: Use models like decision trees or ensemble methods (e.g., Random Forest) to determine\n",
    "    feature importance. The importance can then guide the selection of relevant features for logistic regression.\n",
    "\n",
    "### 5. **Cross-Validation**:\n",
    "   - Perform cross-validation to evaluate the modelâ€™s performance with different subsets of features. This helps ensure \n",
    "    that the selected features contribute to generalization rather than fitting to noise in the training data.\n",
    "\n",
    "### Benefits of Feature Selection\n",
    "\n",
    "1. **Improved Model Performance**:\n",
    "   - By reducing the number of irrelevant or redundant features, the model can focus on the most informative variables,\n",
    "which can lead to better predictive accuracy.\n",
    "\n",
    "2. **Reduced Overfitting**:\n",
    "   - Fewer features mean a simpler model, which is less likely to capture noise in the training data. This improves \n",
    "generalization to unseen data.\n",
    "\n",
    "3. **Enhanced Interpretability**:\n",
    "   - A model with fewer features is easier to interpret, making it simpler to understand how the model makes \n",
    "predictions and identify key drivers behind the outcomes.\n",
    "\n",
    "4. **Decreased Computational Cost**:\n",
    "   - Reducing the number of features can lower the computational burden for model training and prediction, \n",
    "speeding up the overall process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc49b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. How can you handle imbalanced datasets in logistic regression? What are some strategies for dealing\n",
    "with class imbalance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b548606a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Handling imbalanced datasets in logistic regression is crucial for ensuring that the model performs well across all \n",
    "classes, particularly when one class is significantly underrepresented. Here are several strategies to address class\n",
    "imbalance:\n",
    "\n",
    "### 1. **Resampling Techniques**:\n",
    "\n",
    "- **Oversampling**:\n",
    "  - Increase the number of instances in the minority class by duplicating existing examples or generating synthetic \n",
    "examples (e.g., using SMOTE - Synthetic Minority Over-sampling Technique).\n",
    "  \n",
    "- **Undersampling**:\n",
    "  - Reduce the number of instances in the majority class to balance the dataset. This can be done by randomly removing \n",
    "samples, but it may lead to the loss of important information.\n",
    "\n",
    "### 2. **Using Different Evaluation Metrics**:\n",
    "   - Instead of relying solely on accuracy, use metrics that provide better insight into model performance on \n",
    "    imbalanced datasets, such as:\n",
    "     - **Precision**: Proportion of true positives among predicted positives.\n",
    "     - **Recall (Sensitivity)**: Proportion of true positives among actual positives.\n",
    "     - **F1 Score**: The harmonic mean of precision and recall, providing a balance between the two.\n",
    "     - **ROC-AUC**: The area under the ROC curve, which gives an aggregate measure of performance across all \n",
    "        classification thresholds.\n",
    "\n",
    "### 3. **Cost-sensitive Learning**:\n",
    "   - Modify the logistic regression algorithm to take class imbalance into account by assigning a higher cost to \n",
    "    misclassifying the minority class. This can be achieved by:\n",
    "     - Adjusting the class weights in the loss function to penalize mistakes on the minority class more heavily. \n",
    "    Many libraries, like scikit-learn, allow you to set `class_weight='balanced'` in logistic regression.\n",
    "\n",
    "### 4. **Ensemble Methods**:\n",
    "   - Use ensemble techniques that are designed to handle class imbalance:\n",
    "     - **Bagging**: Create multiple subsets of the data, balancing classes in each subset, and train separate models.\n",
    "     - **Boosting**: Focus on misclassified instances from previous iterations (e.g., AdaBoost, Gradient Boosting) to \n",
    "        improve the minority class prediction.\n",
    "\n",
    "### 5. **Threshold Adjustment**:\n",
    "   - Adjust the decision threshold used to classify instances. By default, logistic regression uses a threshold of \n",
    "    0.5 to classify predictions. You can lower this threshold to increase sensitivity to the minority class.\n",
    "\n",
    "### 6. **Data Augmentation**:\n",
    "   - Create synthetic samples for the minority class through techniques like image augmentation (if applicable) or \n",
    "    by perturbing existing data to create new examples.\n",
    "\n",
    "### 7. **Domain Knowledge**:\n",
    "   - Leverage domain knowledge to create new features or identify important patterns that could help improve\n",
    "    classification for the minority class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24057dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. Can you discuss some common issues and challenges that may arise when implementing logistic\n",
    "regression, and how they can be addressed? For example, what can be done if there is multicollinearity\n",
    "among the independent variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29e648a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Implementing logistic regression can come with several challenges and issues that may affect model performance and \n",
    "interpretability. Here are some common problems and their solutions:\n",
    "\n",
    "### 1. **Multicollinearity**:\n",
    "   - **Issue**: Multicollinearity occurs when two or more independent variables are highly correlated, leading to \n",
    "        instability in the estimated coefficients and making it difficult to determine the individual effect of each \n",
    "        variable.\n",
    "   - **Solutions**:\n",
    "     - **Variance Inflation Factor (VIF)**: Calculate VIF for each feature. A VIF value greater than 5 or 10 indicates \n",
    "            high multicollinearity. Consider removing or combining correlated features.\n",
    "     - **Feature Selection**: Use techniques like Lasso (L1 regularization), which can shrink coefficients of less \n",
    "        important features to zero, effectively selecting a subset of features.\n",
    "     - **Principal Component Analysis (PCA)**: Transform the correlated variables into a smaller set of uncorrelated\n",
    "        variables (principal components) that can be used as predictors.\n",
    "\n",
    "### 2. **Overfitting**:\n",
    "   - **Issue**: Logistic regression can overfit the training data, particularly when there are many predictors relative\n",
    "        to the number of observations.\n",
    "   - **Solutions**:\n",
    "     - **Regularization**: Apply L1 (Lasso) or L2 (Ridge) regularization to penalize large coefficients and reduce \n",
    "            model complexity.\n",
    "     - **Cross-Validation**: Use techniques like k-fold cross-validation to assess model performance and ensure it\n",
    "        generalizes well to unseen data.\n",
    "\n",
    "### 3. **Imbalanced Data**:\n",
    "   - **Issue**: Logistic regression can be biased towards the majority class if the dataset is imbalanced, leading to\n",
    "        poor performance on the minority class.\n",
    "   - **Solutions**:\n",
    "     - **Resampling**: Use oversampling, undersampling, or synthetic data generation techniques (like SMOTE) to \n",
    "            balance the classes.\n",
    "     - **Cost-sensitive Learning**: Assign higher penalties for misclassifying the minority class by adjusting class\n",
    "        weights in the logistic regression model.\n",
    "\n",
    "### 4. **Non-Linearity**:\n",
    "   - **Issue**: Logistic regression assumes a linear relationship between the log-odds of the dependent variable and\n",
    "        the independent variables. Non-linear relationships can lead to poor model performance.\n",
    "   - **Solutions**:\n",
    "     - **Feature Engineering**: Create interaction terms or polynomial features to capture non-linear relationships.\n",
    "     - **Transformation**: Apply transformations to the independent variables (e.g., logarithmic or square root) to\n",
    "        better fit the relationship.\n",
    "\n",
    "### 5. **Outliers**:\n",
    "   - **Issue**: Outliers can disproportionately influence the logistic regression model, skewing results and affecting \n",
    "        coefficient estimates.\n",
    "   - **Solutions**:\n",
    "     - **Identification**: Use statistical tests or visualization methods (like box plots) to identify outliers.\n",
    "     - **Handling Outliers**: Consider removing outliers, transforming them, or using robust regression techniques \n",
    "        that are less sensitive to extreme values.\n",
    "\n",
    "### 6. **Sample Size**:\n",
    "   - **Issue**: Small sample sizes can lead to unreliable coefficient estimates and high variance.\n",
    "   - **Solutions**:\n",
    "     - **Increase Sample Size**: If possible, collect more data to improve model robustness.\n",
    "     - **Use Regularization**: Regularization techniques can help mitigate issues with small datasets by stabilizing \n",
    "        coefficient estimates.\n",
    "\n",
    "### 7. **Model Interpretation**:\n",
    "   - **Issue**: While logistic regression provides coefficients that can be interpreted in terms of odds ratios, the \n",
    "        presence of many features or complex interactions can complicate interpretation.\n",
    "   - **Solutions**:\n",
    "     - **Simplification**: Focus on a smaller subset of important features and report results clearly.\n",
    "     - **Visualization**: Use plots (e.g., coefficients plots) to help communicate the relationships and effects of \n",
    "        predictors.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
