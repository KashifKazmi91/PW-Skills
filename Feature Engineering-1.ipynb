{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e7f9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some\n",
    "algorithms that are not affected by missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023c4c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Missing Values in a Dataset\n",
    "\n",
    "**Definition**: Missing values occur when data points are absent for certain features (variables) in a dataset. \n",
    "    This can happen for various reasons, such as errors in data collection, non-responses in surveys, or technical\n",
    "    issues during data entry.\n",
    "\n",
    "### Importance of Handling Missing Values\n",
    "\n",
    "1. **Data Quality**: Missing values can degrade the quality of the dataset, leading to inaccurate or biased analyses.\n",
    "2. **Model Performance**: Many machine learning algorithms cannot handle missing values directly, leading to errors or\n",
    "    suboptimal performance if they encounter them.\n",
    "3. **Bias Prevention**: Ignoring missing values can introduce bias into the model, as the missingness might be related\n",
    "    to the outcome variable.\n",
    "4. **Interpretability**: Incomplete datasets can complicate interpretation, making it difficult to draw valid \n",
    "    conclusions from the analysis.\n",
    "\n",
    "### Common Approaches to Handling Missing Values\n",
    "\n",
    "- **Imputation**: Filling in missing values using statistical methods (mean, median, mode) or predictive modeling\n",
    "    (e.g., using k-NN).\n",
    "- **Removal**: Dropping rows or columns with missing values if they are not significant or if the proportion of\n",
    "    missing data is small.\n",
    "- **Indicator Variables**: Creating binary indicator variables to denote whether a value was missing, allowing \n",
    "    the model to account for missingness explicitly.\n",
    "\n",
    "### Algorithms Not Affected by Missing Values\n",
    "\n",
    "Some algorithms can inherently handle missing values without requiring preprocessing. These include:\n",
    "\n",
    "1. **Tree-Based Algorithms**:\n",
    "   - **Decision Trees**: Can handle missing values by splitting on available data and ignoring missing entries \n",
    "    during training.\n",
    "   - **Random Forests**: Similar to decision trees, they can manage missing values by using surrogate splits.\n",
    "   - **Gradient Boosting Machines (GBM)**: Many implementations, like XGBoost, can handle missing values by \n",
    "    treating them in a special way during training.\n",
    "\n",
    "2. **k-Nearest Neighbors (k-NN)**:\n",
    "   - While it requires a complete dataset for distance calculations, some implementations allow handling missing\n",
    "values by ignoring them or by filling them based on neighbors.\n",
    "\n",
    "3. **Naive Bayes**:\n",
    "   - Certain implementations can handle missing values by simply omitting the missing features in the probability\n",
    "calculations.\n",
    "\n",
    "4. **Support Vector Machines (SVM)**:\n",
    "   - Some implementations can handle missing values by excluding them during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9885fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. **Removing Missing Data**\n",
    "\n",
    "#### Example: Dropping Rows with Missing Values\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame with missing values\n",
    "data = {'A': [1, 2, None, 4],\n",
    "        'B': [None, 2, 3, 4],\n",
    "        'C': [1, 2, 3, 4]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Dropping rows with any missing values\n",
    "df_dropped = df.dropna()\n",
    "print(df_dropped)\n",
    "```\n",
    "\n",
    "### 2. **Mean/Median/Mode Imputation**\n",
    "\n",
    "#### Example: Filling Missing Values with Mean\n",
    "\n",
    "```python\n",
    "# Filling missing values with the mean of the column\n",
    "df['A'].fillna(df['A'].mean(), inplace=True)\n",
    "print(df)\n",
    "```\n",
    "\n",
    "### 3. **Forward Fill and Backward Fill**\n",
    "\n",
    "#### Example: Forward Fill\n",
    "\n",
    "```python\n",
    "# Forward filling missing values\n",
    "df['B'].fillna(method='ffill', inplace=True)\n",
    "print(df)\n",
    "```\n",
    "\n",
    "### 4. **K-Nearest Neighbors (k-NN) Imputation**\n",
    "\n",
    "#### Example: Using KNN Imputer from Scikit-learn\n",
    "\n",
    "```python\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {'A': [1, 2, None, 4],\n",
    "        'B': [None, 2, 3, 4],\n",
    "        'C': [1, 2, 3, 4]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create a KNN imputer\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "\n",
    "# Impute missing values\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "df_imputed = pd.DataFrame(df_imputed, columns=df.columns)\n",
    "print(df_imputed)\n",
    "```\n",
    "\n",
    "### 5. **Multiple Imputation**\n",
    "\n",
    "#### Example: Using the IterativeImputer\n",
    "\n",
    "```python\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {'A': [1, 2, None, 4],\n",
    "        'B': [None, 2, 3, 4],\n",
    "        'C': [1, 2, 3, 4]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create an Iterative Imputer\n",
    "imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "\n",
    "# Impute missing values\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "df_imputed = pd.DataFrame(df_imputed, columns=df.columns)\n",
    "print(df_imputed)\n",
    "```\n",
    "\n",
    "### 6. **Using a Constant Value**\n",
    "\n",
    "#### Example: Filling Missing Values with a Constant\n",
    "\n",
    "```python\n",
    "# Filling missing values with a constant value\n",
    "df['A'].fillna(0, inplace=True)  # Replace NaN in column A with 0\n",
    "print(df)\n",
    "```\n",
    "\n",
    "### 7. **Creating Indicator Variables for Missing Data**\n",
    "\n",
    "#### Example: Adding a Missing Indicator\n",
    "\n",
    "```python\n",
    "# Creating an indicator variable for missing values\n",
    "df['B_missing'] = df['B'].isnull().astype(int)\n",
    "print(df)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481bdbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764b133a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imbalanced Data\n",
    "\n",
    "**Definition**: Imbalanced data refers to a situation in a classification problem where the distribution of classes \n",
    "    is not uniform. Specifically, one class (the majority class) significantly outnumbers the other class \n",
    "    (the minority class). For example, in a binary classification task, if 90% of the data points belong to \n",
    "    class A and only 10% belong to class B, the dataset is considered imbalanced.\n",
    "\n",
    "### Consequences of Not Handling Imbalanced Data\n",
    "\n",
    "1. **Biased Model Performance**:\n",
    "   - The model may become biased toward the majority class, leading to high accuracy but poor performance on the\n",
    "minority class. For instance, if a model predicts every instance as the majority class, it might still achieve high\n",
    "accuracy, but it fails to identify any instances of the minority class.\n",
    "\n",
    "2. **High False Negatives**:\n",
    "   - The model is likely to generate a high number of false negatives for the minority class. In applications like\n",
    "fraud detection or disease diagnosis, failing to identify minority class instances can have serious implications.\n",
    "\n",
    "3. **Poor Generalization**:\n",
    "   - Models trained on imbalanced datasets may not generalize well to new data, particularly if the new data reflects\n",
    "a more balanced distribution. This can lead to significant performance degradation in real-world scenarios.\n",
    "\n",
    "4. **Misleading Evaluation Metrics**:\n",
    "   - Standard metrics like accuracy can be misleading in the context of imbalanced datasets. A high accuracy could\n",
    "mask the poor performance on the minority class. Instead, metrics such as precision, recall, F1-score, and the area\n",
    "under the ROC curve (AUC-ROC) should be used to evaluate model performance.\n",
    "\n",
    "5. **Overfitting to Majority Class**:\n",
    "   - The model may learn to optimize for the majority class, overfitting to its patterns and ignoring the minority \n",
    "class entirely. This can reduce the model's overall effectiveness.\n",
    "\n",
    "### Importance of Handling Imbalanced Data\n",
    "\n",
    "To mitigate the issues associated with imbalanced datasets, various techniques can be employed:\n",
    "\n",
    "1. **Resampling Techniques**:\n",
    "   - **Oversampling**: Increase the number of instances in the minority class (e.g., using techniques like \n",
    "    SMOTEâ€”Synthetic Minority Over-sampling Technique).\n",
    "   - **Undersampling**: Reduce the number of instances in the majority class to balance the dataset.\n",
    "\n",
    "2. **Algorithmic Adjustments**:\n",
    "   - Use algorithms that are robust to imbalanced data, such as tree-based models that can inherently handle\n",
    "imbalance better.\n",
    "   - Implement cost-sensitive learning, where different misclassification costs are assigned to different classes.\n",
    "\n",
    "3. **Ensemble Methods**:\n",
    "   - Use ensemble techniques like boosting and bagging to improve the performance of classifiers on imbalanced datasets.\n",
    "\n",
    "4. **Evaluation Metrics**:\n",
    "   - Focus on using metrics that provide a better sense of performance on both classes, such as precision, recall,\n",
    "F1-score, and ROC-AUC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0580b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-\n",
    "sampling are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2ebb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Up-sampling and Down-sampling\n",
    "\n",
    "**Up-sampling** and **down-sampling** are techniques used to address class imbalance in datasets, particularly in \n",
    "classification tasks.\n",
    "\n",
    "#### Up-sampling\n",
    "\n",
    "**Definition**: Up-sampling, also known as oversampling, involves increasing the number of instances in the minority\n",
    "    class to create a more balanced dataset. This can be done by duplicating existing instances or generating synthetic\n",
    "    samples.\n",
    "\n",
    "**When to Use**: Up-sampling is typically required when the minority class is significantly underrepresented in the\n",
    "    dataset, and you want to ensure that the model has enough examples to learn from.\n",
    "\n",
    "**Example**:\n",
    "- Suppose you have a binary classification dataset with the following distribution:\n",
    "  - Majority class (Class 0): 90 samples\n",
    "  - Minority class (Class 1): 10 samples\n",
    "\n",
    "Using up-sampling, you could randomly duplicate instances of Class 1 until you have, say, 90 instances of both \n",
    "classes:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {'Class': [0]*90 + [1]*10}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_majority = df[df['Class'] == 0]\n",
    "df_minority = df[df['Class'] == 1]\n",
    "\n",
    "# Up-sample minority class\n",
    "df_minority_upsampled = resample(df_minority,\n",
    "                                  replace=True,     # Sample with replacement\n",
    "                                  n_samples=90,     # To match majority class\n",
    "                                  random_state=42)  # Reproducible results\n",
    "\n",
    "# Combine majority class with upsampled minority class\n",
    "df_balanced = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "print(df_balanced['Class'].value_counts())\n",
    "```\n",
    "\n",
    "#### Down-sampling\n",
    "\n",
    "**Definition**: Down-sampling, also known as undersampling, involves reducing the number of instances in the majority\n",
    "    class to create a more balanced dataset. This can help to mitigate the risk of the model being biased toward the\n",
    "    majority class.\n",
    "\n",
    "**When to Use**: Down-sampling is often required when the dataset is large, and the majority class is excessively \n",
    "    represented, potentially leading to overfitting.\n",
    "\n",
    "**Example**:\n",
    "- Continuing with the previous dataset, if you have:\n",
    "  - Majority class (Class 0): 90 samples\n",
    "  - Minority class (Class 1): 10 samples\n",
    "\n",
    "Using down-sampling, you could randomly select a subset of the majority class to reduce its size to match that of the\n",
    "minority class (10 samples):\n",
    "\n",
    "```python\n",
    "# Down-sample majority class\n",
    "df_majority_downsampled = resample(df_majority,\n",
    "                                    replace=False,    # Sample without replacement\n",
    "                                    n_samples=10,     # To match minority class\n",
    "                                    random_state=42)  # Reproducible results\n",
    "\n",
    "# Combine downsampled majority class with minority class\n",
    "df_balanced_down = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "print(df_balanced_down['Class'].value_counts())\n",
    "``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e6b7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5: What is data Augmentation? Explain SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddcb4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Augmentation\n",
    "\n",
    "**Definition**: Data augmentation is a technique used to artificially increase the size of a training dataset by\n",
    "    creating modified versions of existing data points. This is particularly common in fields like computer vision\n",
    "    and natural language processing, where collecting more data can be expensive or impractical.\n",
    "\n",
    "**Purpose**: The main goals of data augmentation are to:\n",
    "- Improve model generalization by introducing variability in the training data.\n",
    "- Reduce overfitting by exposing the model to a wider range of inputs.\n",
    "\n",
    "**Common Techniques**:\n",
    "- **For Images**: Rotation, scaling, flipping, cropping, adding noise, and color adjustments.\n",
    "- **For Text**: Synonym replacement, random insertion of words, and back-translation.\n",
    "\n",
    "### SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "\n",
    "**Definition**: SMOTE is a specific type of data augmentation technique used to address class imbalance in datasets.\n",
    "    It generates synthetic samples for the minority class by interpolating between existing samples.\n",
    "\n",
    "**How SMOTE Works**:\n",
    "1. **Identify Minority Instances**: For each instance in the minority class, SMOTE identifies its k-nearest neighbors\n",
    "    (usually k=5).\n",
    "2. **Create Synthetic Instances**: For each selected neighbor, a synthetic instance is created by interpolating between\n",
    "    the minority instance and its neighbor. This involves selecting a random point along the line segment joining the\n",
    "    two instances.\n",
    "\n",
    "   \\[\n",
    "   \\text{Synthetic Instance} = \\text{Instance}_i + \\lambda \\times (\\text{Neighbor}_j - \\text{Instance}_i)\n",
    "   \\]\n",
    "\n",
    "   where \\( \\lambda \\) is a random number between 0 and 1.\n",
    "\n",
    "3. **Repeat**: This process continues until the desired number of synthetic samples is generated.\n",
    "\n",
    "### Example of SMOTE in Python\n",
    "\n",
    "Here's how you can implement SMOTE using the `imbalanced-learn` library in Python:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Create a sample imbalanced dataset\n",
    "X, y = make_classification(n_classes=2, n_informative=3, n_redundant=1,\n",
    "                           weights=[0.9, 0.1], n_samples=1000, random_state=42)\n",
    "\n",
    "# Convert to DataFrame for visualization\n",
    "df = pd.DataFrame(X)\n",
    "df['target'] = y\n",
    "\n",
    "# Check the original class distribution\n",
    "print(\"Original class distribution:\")\n",
    "print(df['target'].value_counts())\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Convert the resampled data to DataFrame\n",
    "df_resampled = pd.DataFrame(X_resampled)\n",
    "df_resampled['target'] = y_resampled\n",
    "\n",
    "# Check the new class distribution\n",
    "print(\"\\nNew class distribution after SMOTE:\")\n",
    "print(df_resampled['target'].value_counts())\n",
    "``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681f26b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6: What are outliers in a dataset? Why is it essential to handle outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107aad4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Outliers in a Dataset\n",
    "\n",
    "**Definition**: Outliers are data points that significantly differ from the majority of the observations in a dataset. \n",
    "    They can be unusually high or low values that do not fit the expected pattern or distribution of the data.\n",
    "\n",
    "### Why Itâ€™s Essential to Handle Outliers\n",
    "\n",
    "1. **Impact on Statistical Analysis**:\n",
    "   - Outliers can skew statistical measures such as the mean and standard deviation, leading to misleading \n",
    "interpretations. For example, a few extremely high values can raise the mean, suggesting a higher central tendency\n",
    "than is representative of the bulk of the data.\n",
    "\n",
    "2. **Influence on Model Performance**:\n",
    "   - In machine learning, outliers can adversely affect model training. Many algorithms, such as linear regression,\n",
    "are sensitive to outliers and may produce poor predictions as a result. Outliers can distort the decision boundaries\n",
    "and lead to overfitting or underfitting.\n",
    "\n",
    "3. **Assumptions of Statistical Methods**:\n",
    "   - Many statistical methods assume that the data follows a certain distribution (e.g., normal distribution). \n",
    "Outliers can violate these assumptions, affecting hypothesis testing and confidence intervals.\n",
    "\n",
    "4. **Data Quality and Integrity**:\n",
    "   - Outliers may indicate errors in data collection or entry (e.g., measurement errors, data corruption). \n",
    "Identifying and addressing outliers helps ensure data quality and integrity.\n",
    "\n",
    "5. **Real-World Implications**:\n",
    "   - In some contexts, outliers can represent significant events or phenomena (e.g., fraud detection, rare diseases).\n",
    "While handling outliers is essential, it is equally important to determine whether they hold valuable information that\n",
    "should be preserved for analysis.\n",
    "\n",
    "### Handling Outliers\n",
    "\n",
    "Handling outliers can involve several approaches:\n",
    "\n",
    "1. **Identification**:\n",
    "   - Use statistical methods (e.g., z-scores, IQR) or visualization techniques (e.g., box plots, scatter plots) to\n",
    "identify outliers.\n",
    "\n",
    "2. **Removal**:\n",
    "   - In some cases, it may be appropriate to remove outliers if they are deemed to be errors or do not contribute\n",
    "meaningful information.\n",
    "\n",
    "3. **Transformation**:\n",
    "   - Apply transformations (e.g., logarithmic transformation) to reduce the impact of outliers on the overall dataset.\n",
    "\n",
    "4. **Imputation**:\n",
    "   - Replace outlier values with more representative values, such as the median or a calculated value based on other\n",
    "data points.\n",
    "\n",
    "5. **Model Robustness**:\n",
    "   - Use algorithms that are less sensitive to outliers (e.g., tree-based methods) when appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579fbc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7: You are working on a project that requires analyzing customer data. However, you notice that some of\n",
    "the data is missing. What are some techniques you can use to handle the missing data in your analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc172c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Handling missing data is crucial for ensuring the accuracy and reliability of your analysis. Here are some techniques \n",
    "you can use to address missing data in customer analysis:\n",
    "\n",
    "### 1. **Removal of Missing Data**\n",
    "\n",
    "- **Dropping Rows**: If the missing data is limited to a small number of rows and isn't critical, you can remove those\n",
    "    rows from the dataset.\n",
    "  \n",
    "  ```python\n",
    "  df_cleaned = df.dropna()  # Drops any rows with missing values\n",
    "  ```\n",
    "\n",
    "- **Dropping Columns**: If an entire column has a high percentage of missing values and is not essential for your\n",
    "    analysis, you can drop the column.\n",
    "  \n",
    "  ```python\n",
    "  df_cleaned = df.drop(columns=['column_with_many_nans'])  # Replace with actual column name\n",
    "  ```\n",
    "\n",
    "### 2. **Imputation Techniques**\n",
    "\n",
    "- **Mean/Median/Mode Imputation**: Fill missing values with the mean (for numerical data), median (to reduce the \n",
    "influence of outliers), or mode (for categorical data) of the respective columns.\n",
    "  \n",
    "  ```python\n",
    "  df['numerical_column'].fillna(df['numerical_column'].mean(), inplace=True)  # Mean imputation\n",
    "  df['categorical_column'].fillna(df['categorical_column'].mode()[0], inplace=True)  # Mode imputation\n",
    "  ```\n",
    "\n",
    "- **Forward Fill / Backward Fill**: Use the previous or next value to fill in missing entries, particularly useful \n",
    "    for time-series data.\n",
    "  \n",
    "  ```python\n",
    "  df.fillna(method='ffill', inplace=True)  # Forward fill\n",
    "  ```\n",
    "\n",
    "### 3. **K-Nearest Neighbors (k-NN) Imputation**\n",
    "\n",
    "- Use the k-NN algorithm to fill missing values based on the values of the nearest neighbors.\n",
    "  \n",
    "  ```python\n",
    "  from sklearn.impute import KNNImputer\n",
    "\n",
    "  imputer = KNNImputer(n_neighbors=5)\n",
    "  df_imputed = imputer.fit_transform(df)\n",
    "  ```\n",
    "\n",
    "### 4. **Multiple Imputation**\n",
    "\n",
    "- Create several different plausible imputed datasets and combine the results to account for uncertainty in the\n",
    "imputations.\n",
    "\n",
    "  ```python\n",
    "  from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "  from sklearn.impute import IterativeImputer\n",
    "\n",
    "  imputer = IterativeImputer()\n",
    "  df_imputed = imputer.fit_transform(df)\n",
    "  ```\n",
    "\n",
    "### 5. **Using Indicator Variables**\n",
    "\n",
    "- Create a binary indicator for missing values to retain the information about which values were missing. \n",
    "This can help the model capture patterns related to missingness.\n",
    "\n",
    "  ```python\n",
    "  df['missing_indicator'] = df['numerical_column'].isnull().astype(int)\n",
    "  ```\n",
    "\n",
    "### 6. **Model-Based Imputation**\n",
    "\n",
    "- Use a machine learning model to predict missing values based on other available data. For example, use regression \n",
    "models to predict missing values in numerical columns based on other features.\n",
    "\n",
    "### 7. **Domain-Specific Methods**\n",
    "\n",
    "- Sometimes, domain knowledge can help determine the best way to handle missing data. For example, if customer age is\n",
    "missing, you might impute it based on the average age of similar customer segments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572d643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are\n",
    "some strategies you can use to determine if the missing data is missing at random or if there is a pattern\n",
    "to the missing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3683c5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Determining whether missing data is missing at random (MAR), missing completely at random (MCAR), or missing not at \n",
    "random (MNAR) is crucial for choosing the appropriate handling strategy. Here are some strategies to assess the nature\n",
    "of the missing data:\n",
    "\n",
    "### 1. **Visual Inspection**\n",
    "\n",
    "- **Missing Data Patterns**: Use heatmaps or missing data matrices (like those from the `missingno` library in Python)\n",
    "    to visualize patterns of missingness. Look for any patterns or correlations with specific features.\n",
    "  \n",
    "  ```python\n",
    "  import missingno as msno\n",
    "  msno.matrix(df)  # Visualizes missing values in the DataFrame\n",
    "  ```\n",
    "\n",
    "- **Box Plots**: Create box plots to examine if the distribution of observed data varies between those with missing\n",
    "    values and those without.\n",
    "\n",
    "### 2. **Statistical Tests**\n",
    "\n",
    "- **Little's MCAR Test**: This test can help determine if the data is missing completely at random. The null \n",
    "hypothesis is that the data is MCAR. If you fail to reject the null hypothesis, the missingness can be considered MCAR.\n",
    "  \n",
    "  ```python\n",
    "  from statsmodels.stats.missing import LittleMCAR\n",
    "\n",
    "  result = LittleMCAR(df)\n",
    "  print(result)\n",
    "  ```\n",
    "\n",
    "### 3. **Correlation Analysis**\n",
    "\n",
    "- **Correlation with Missingness**: Create binary indicators for missing values in each feature and check for \n",
    "    correlations with other features. High correlations may indicate that the missingness is related to specific\n",
    "    variables.\n",
    "\n",
    "  ```python\n",
    "  missing_indicators = df.isnull().astype(int)\n",
    "  correlation_matrix = df.corr().join(missing_indicators.corr())\n",
    "  ```\n",
    "\n",
    "### 4. **Compare Groups**\n",
    "\n",
    "- **Group Comparisons**: Compare statistics (means, medians) of different groups in your dataset based on whether \n",
    "    data is missing. Significant differences between groups may suggest that the missingness is related to the \n",
    "    underlying data.\n",
    "\n",
    "### 5. **Predictive Modeling**\n",
    "\n",
    "- **Predicting Missing Values**: Build a model to predict whether data is missing based on other features. If the \n",
    "    model can predict missingness well, it suggests that the missing data is not random (MNAR or MAR).\n",
    "\n",
    "  ```python\n",
    "  from sklearn.ensemble import RandomForestClassifier\n",
    "  from sklearn.model_selection import train_test_split\n",
    "\n",
    "  df['missing_feature'] = df['target_column'].isnull().astype(int)\n",
    "  X = df.drop(columns=['target_column', 'missing_feature'])\n",
    "  y = df['missing_feature']\n",
    "  \n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "  \n",
    "  model = RandomForestClassifier()\n",
    "  model.fit(X_train, y_train)\n",
    "  print(model.score(X_test, y_test))\n",
    "  ```\n",
    "\n",
    "### 6. **Examine Time or Order Effects**\n",
    "\n",
    "- **Temporal Analysis**: If your data is time-series or ordered, analyze whether missing data occurs at certain \n",
    "    times or conditions. For example, data might be more likely to be missing during specific time periods or events.\n",
    "\n",
    "### 7. **Domain Knowledge**\n",
    "\n",
    "- **Consult Domain Experts**: Leverage domain knowledge to understand potential reasons for missingness. Experts may\n",
    "    provide insights into whether certain features are likely to have missing values due to specific processes or \n",
    "    behaviors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb44936",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the\n",
    "dataset do not have the condition of interest, while a small percentage do. What are some strategies you\n",
    "can use to evaluate the performance of your machine learning model on this imbalanced dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11c3628",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluating the performance of a machine learning model on an imbalanced dataset, especially in critical applications \n",
    "like medical diagnosis, requires careful consideration of metrics and strategies. Here are some effective approaches:\n",
    "\n",
    "### 1. **Use Appropriate Evaluation Metrics**\n",
    "\n",
    "- **Precision**: Measures the accuracy of positive predictions. It is particularly important when the cost of false \n",
    "    positives is high.\n",
    "  \n",
    "  \\[\n",
    "  \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
    "  \\]\n",
    "\n",
    "- **Recall (Sensitivity)**: Measures the ability of the model to identify positive cases. This is crucial in medical \n",
    "    diagnosis to minimize false negatives.\n",
    "\n",
    "  \\[\n",
    "  \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
    "  \\]\n",
    "\n",
    "- **F1-Score**: The harmonic mean of precision and recall, useful when you need a balance between the two.\n",
    "\n",
    "  \\[\n",
    "  \\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "  \\]\n",
    "\n",
    "- **Area Under the Receiver Operating Characteristic Curve (ROC-AUC)**: This metric evaluates the model's performance\n",
    "    across different thresholds and provides a single score to summarize the model's ability to discriminate between \n",
    "    classes.\n",
    "\n",
    "- **Area Under the Precision-Recall Curve (PR-AUC)**: Particularly useful for imbalanced datasets, focusing on the \n",
    "    performance of the model with respect to the positive class.\n",
    "\n",
    "### 2. **Confusion Matrix**\n",
    "\n",
    "- Analyze the confusion matrix to understand how many true positives, true negatives, false positives, and false\n",
    "negatives your model is producing. This detailed breakdown can help you identify specific areas of improvement.\n",
    "\n",
    "### 3. **Cross-Validation with Stratification**\n",
    "\n",
    "- Use stratified cross-validation to ensure that each fold of your training and validation sets has a similar \n",
    "proportion of classes. This helps maintain the distribution of the minority class during training and evaluation.\n",
    "\n",
    "### 4. **Resampling Techniques**\n",
    "\n",
    "- **Upsampling**: Increase the number of instances of the minority class.\n",
    "- **Downsampling**: Decrease the number of instances of the majority class.\n",
    "- **SMOTE**: Use Synthetic Minority Over-sampling Technique to generate synthetic samples for the minority class.\n",
    "\n",
    "### 5. **Cost-Sensitive Learning**\n",
    "\n",
    "- Implement cost-sensitive algorithms that assign different misclassification costs for different classes. \n",
    "This approach helps the model focus on minimizing errors for the minority class.\n",
    "\n",
    "### 6. **Ensemble Methods**\n",
    "\n",
    "- Use ensemble techniques like Random Forests, Gradient Boosting, or even specific algorithms designed for imbalanced\n",
    "data (e.g., Balanced Random Forest). These methods can often provide better performance by combining the strengths of \n",
    "multiple models.\n",
    "\n",
    "### 7. **Threshold Tuning**\n",
    "\n",
    "- Adjust the decision threshold used for classification. Instead of using the default threshold (often 0.5), \n",
    "evaluate different thresholds based on precision-recall trade-offs or the ROC curve to optimize for sensitivity \n",
    "or specificity based on clinical needs.\n",
    "\n",
    "### 8. **Monitoring and Validation in Real-World Scenarios**\n",
    "\n",
    "- If possible, validate model predictions against clinical outcomes in a real-world setting. This can provide \n",
    "valuable insights into how the model performs in practice and can help adjust your evaluation metrics accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bb35cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is\n",
    "unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to\n",
    "balance the dataset and down-sample the majority class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccf5a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "Balancing an unbalanced dataset, especially in a scenario like estimating customer satisfaction where most customers\n",
    "report being satisfied, is important to ensure that your model can effectively learn to identify the minority class \n",
    "(e.g., dissatisfied customers). Here are some methods to balance the dataset and down-sample the majority class:\n",
    "\n",
    "### 1. **Random Undersampling**\n",
    "\n",
    "Randomly remove samples from the majority class until the desired balance with the minority class is achieved. \n",
    "This is the simplest method but can lead to loss of valuable information.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Assuming df is your DataFrame and 'satisfaction' is the target column\n",
    "df_majority = df[df['satisfaction'] == 'satisfied']\n",
    "df_minority = df[df['satisfaction'] == 'dissatisfied']\n",
    "\n",
    "# Down-sample majority class\n",
    "df_majority_downsampled = resample(df_majority,\n",
    "                                    replace=False,    # Sample without replacement\n",
    "                                    n_samples=len(df_minority),  # Match minority class size\n",
    "                                    random_state=42)  # Reproducible results\n",
    "\n",
    "# Combine downsampled majority class with minority class\n",
    "df_balanced = pd.concat([df_majority_downsampled, df_minority])\n",
    "```\n",
    "\n",
    "### 2. **Cluster-Based Undersampling**\n",
    "\n",
    "Instead of random undersampling, use clustering techniques (like K-means) to group the majority class and then \n",
    "select a representative sample from each cluster. This can help retain diversity in the remaining samples.\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Assuming you have feature columns in X\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "df_majority['cluster'] = kmeans.fit_predict(df_majority.drop(columns=['satisfaction']))\n",
    "\n",
    "# Select one sample from each cluster\n",
    "df_majority_downsampled = df_majority.groupby('cluster').apply(lambda x: x.sample(1)).reset_index(drop=True)\n",
    "```\n",
    "\n",
    "### 3. **Tomek Links and Edited Nearest Neighbors (ENN)**\n",
    "\n",
    "Tomek Links and ENN are techniques that help refine the majority class by removing samples that are close to the \n",
    "minority class. This can help clarify the decision boundary.\n",
    "\n",
    "- **Tomek Links**: Identify pairs of samples that are nearest neighbors but belong to different classes and remove \n",
    "    the majority class member.\n",
    "- **ENN**: Similar to Tomek Links, but you consider the majority class neighbors and remove samples that have a \n",
    "    majority of neighbors in the minority class.\n",
    "\n",
    "### 4. **NearMiss**\n",
    "\n",
    "NearMiss is a specific method of undersampling that involves selecting samples from the majority class based on their\n",
    "distances to the minority class samples. This technique helps retain important examples from the majority class.\n",
    "\n",
    "### 5. **Use of Synthetic Data Generation**\n",
    "\n",
    "While the focus here is on down-sampling, consider using methods like SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "to create synthetic instances of the minority class, alongside down-sampling the majority class. This can help maintain\n",
    "the overall dataset size while balancing the classes.\n",
    "\n",
    "### 6. **Stratified Sampling**\n",
    "\n",
    "If you need to perform cross-validation or create training and test sets, use stratified sampling to ensure that each\n",
    "split maintains the original distribution of classes.\n",
    "\n",
    "### 7. **Cost-Sensitive Learning**\n",
    "\n",
    "Instead of balancing the dataset, you can also modify the learning algorithm to assign higher costs to misclassifying \n",
    "the minority class. This approach encourages the model to pay more attention to the minority class without changing the\n",
    "dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c260001",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a\n",
    "project that requires you to estimate the occurrence of a rare event. What methods can you employ to\n",
    "balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c08f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "When working with an unbalanced dataset that has a low percentage of occurrences for a rare event, up-sampling\n",
    "the minority class can help improve model performance. Here are several effective methods to up-sample the minority\n",
    "class:\n",
    "\n",
    "### 1. **Random Oversampling**\n",
    "\n",
    "Randomly duplicate instances from the minority class until it reaches a desired size. This is straightforward but can\n",
    "lead to overfitting since it simply replicates existing samples.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Assuming df is your DataFrame and 'target' is the binary classification column\n",
    "df_minority = df[df['target'] == 'rare_event']\n",
    "df_majority = df[df['target'] == 'non_event']\n",
    "\n",
    "# Randomly up-sample minority class\n",
    "df_minority_upsampled = resample(df_minority,\n",
    "                                  replace=True,     # Sample with replacement\n",
    "                                  n_samples=len(df_majority),  # To match majority class size\n",
    "                                  random_state=42)  # Reproducible results\n",
    "\n",
    "# Combine majority class with upsampled minority class\n",
    "df_balanced = pd.concat([df_majority, df_minority_upsampled])\n",
    "```\n",
    "\n",
    "### 2. **Synthetic Minority Over-sampling Technique (SMOTE)**\n",
    "\n",
    "SMOTE generates synthetic examples rather than duplicating existing ones. It works by selecting a minority instance\n",
    "and creating new synthetic instances along the line segments between the selected instance and its nearest neighbors.\n",
    "\n",
    "```python\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Assuming X is your feature matrix and y is your target variable\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "```\n",
    "\n",
    "### 3. **Adaptive Synthetic Sampling (ADASYN)**\n",
    "\n",
    "ADASYN is an extension of SMOTE that focuses on generating synthetic data for minority instances that are harder to\n",
    "classify. It adapts the number of synthetic samples to generate based on the difficulty of classifying the minority\n",
    "instances.\n",
    "\n",
    "```python\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "adasyn = ADASYN(random_state=42)\n",
    "X_resampled, y_resampled = adasyn.fit_resample(X, y)\n",
    "```\n",
    "\n",
    "### 4. **Borderline-SMOTE**\n",
    "\n",
    "Borderline-SMOTE is a variation of SMOTE that specifically focuses on generating synthetic instances for minority\n",
    "class samples that are near the decision boundary, making it particularly effective in distinguishing rare events.\n",
    "\n",
    "### 5. **Cluster-Based Oversampling**\n",
    "\n",
    "Cluster the minority class instances and then apply SMOTE or random oversampling to each cluster. This can help retain\n",
    "diversity in the samples generated.\n",
    "\n",
    "### 6. **Ensemble Methods**\n",
    "\n",
    "Use ensemble techniques that can handle imbalanced datasets better. For example, **Balanced Random Forest** and \n",
    "**EasyEnsemble** combine undersampling and oversampling to improve model robustness.\n",
    "\n",
    "### 7. **Cost-Sensitive Learning**\n",
    "\n",
    "Adjust the algorithm to penalize misclassifications of the minority class more heavily. This can be done through \n",
    "custom loss functions in models that support it (e.g., using class weights in logistic regression or decision trees).\n",
    "\n",
    "### 8. **Data Augmentation**\n",
    "\n",
    "For certain types of data (e.g., images, text), apply data augmentation techniques to generate more diverse examples\n",
    "of the minority class. This can include transformations like rotations, translations, or noise addition in image data,\n",
    "or synonym replacement in text data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e73a7a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
